{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "import matplotlib as mpl\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import spearmanr, pearsonr, entropy\n",
    "import libpysal\n",
    "from esda.moran import Moran\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import scipy.stats\n",
    "from fastcluster import linkage\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from matplotlib.patches import Ellipse\n",
    "import scipy.linalg as linalg\n",
    "import numpy.linalg as la\n",
    "import seaborn as sns\n",
    "from itertools import combinations\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "from Functions import *\n",
    "import matplotlib.lines as mlines\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assess how \"sensitive\" your autoencoder is to its initialization and get a sense of how stable the learned embeddings are across different training runs. This approach is especially valuable in research or in situations where reproducibility and stability of the model are crucial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def box_plot(dictionary, var_name, value_name, save_title, box_width=0.8, xlabel_rot=0):\n",
    "\n",
    "    # Make dataframe from dictionary\n",
    "    df = pd.DataFrame(dictionary)\n",
    "\n",
    "    # Melt the DataFrame to reshape it\n",
    "    df = pd.melt(df, var_name=var_name, value_name=value_name)\n",
    "\n",
    "    mpl.rcParams['figure.dpi'] = 500\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.boxplot(data = df, x=var_name, y=value_name, boxprops=dict(alpha=.9),palette=\"muted\",linewidth=0.7,fliersize=0.9, width=box_width)\n",
    "    plt.ylabel(value_name,fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(rotation=xlabel_rot)\n",
    "    plt.savefig(save_title+'.tiff', dpi=300, bbox_inches='tight')\n",
    "    plt.show(block=False)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def percentage_change(measure, data_type):\n",
    "\n",
    "    \"\"\"\n",
    "    Finds the percentage change in any measure of choice for each consecutive pair of AE realizations\n",
    "    :param realizations:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "\n",
    "    if data_type.lower() == 'list':\n",
    "        for i in range(0, len(measure)):\n",
    "            prev_set = set(measure[i - 1])\n",
    "            current_set = set(measure[i])\n",
    "            changed_elements = current_set.symmetric_difference(prev_set)\n",
    "            total_elements = len(prev_set.union(current_set))\n",
    "            change = len(changed_elements) / total_elements * 100\n",
    "            changes.append(change)\n",
    "\n",
    "    elif data_type.lower() == 'numpy':\n",
    "        for i in range(0, len(measure)):\n",
    "            prev_row = measure[i - 1]\n",
    "            current_row = measure[i]\n",
    "            change = np.abs((current_row - prev_row) / prev_row) * 100\n",
    "            changes.append(change)\n",
    "\n",
    "    return changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this function is a sequential comparison and not comprehensive across all AE realization pairs. If you want a global stability/instability view, then consider the Jaccard similarity instead or MC approach.\n",
    "\n",
    "The sequential change in percentages is useful for understanding the immediate stability of the model from one realization to the next. It can highlight how sensitive the model is to small changes in the random seed or initialization parameters. On the other hand, the Jaccard similarity across all pairs of realizations offers a broader view of overall stability and consistency across the entire range of realizations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"\n",
    "    This function checks if the distance matrix is symmetric, prior to making a sorted dissimilarity matrix\n",
    "    \"\"\"\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "\n",
    "def seriation(Z, N, cur_index):\n",
    "    \"\"\"\n",
    "    This is a function that creates a sorted 2D matrix as a figure\n",
    "\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "\n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    \"\"\"\n",
    "\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index - N, 0])\n",
    "        right = int(Z[cur_index - N, 1])\n",
    "        return seriation(Z, N, left) + seriation(Z, N, right)\n",
    "\n",
    "\n",
    "def compute_serial_matrix(dist_mat, method=\"ward\"):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            -  = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarchical tree\n",
    "            - res_linkage is the hierarchical tree (dendrogram)\n",
    "\n",
    "        compute_serial_matrix transforms a distance matrix into\n",
    "        a sorted distance matrix according to the order implied\n",
    "        by the hierarchical tree (dendrogram)\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = dist_mat if len(dist_mat.shape) == 2 else squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method, preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N - 2)\n",
    "    seriated_dist = np.zeros((N, N))\n",
    "    a, b = np.triu_indices(N, k=1)\n",
    "    seriated_dist[a, b] = dist_mat[[res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b, a] = seriated_dist[a, b]\n",
    "\n",
    "    return seriated_dist, res_order, res_linkage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mvee(points, tol=0.0001):\n",
    "    \"\"\"\n",
    "    Finds the ellipse equation in \"center form\"\n",
    "    (x-c).T * A * (x-c) = 1\n",
    "    \"\"\"\n",
    "    N, d = points.shape\n",
    "    Q = np.column_stack((points, np.ones(N))).T\n",
    "    err = tol+1.0\n",
    "    u = np.ones(N)/N\n",
    "    while err > tol:\n",
    "        # assert u.sum() == 1 # invariant\n",
    "        X = np.dot(np.dot(Q, np.diag(u)), Q.T)\n",
    "        M = np.diag(np.dot(np.dot(Q.T, la.inv(X)), Q))\n",
    "        jdx = np.argmax(M)\n",
    "        step_size = (M[jdx]-d-1.0)/((d+1)*(M[jdx]-1.0))\n",
    "        new_u = (1-step_size)*u\n",
    "        new_u[jdx] += step_size\n",
    "        err = la.norm(new_u-u)\n",
    "        u = new_u\n",
    "    c = np.dot(u, points)\n",
    "    A = la.inv(np.dot(np.dot(points.T, np.diag(u)), points)\n",
    "               - np.multiply.outer(c, c))/d\n",
    "    return A, c\n",
    "\n",
    "\n",
    "def dist_2_cent(x, y, center):\n",
    "    '''\n",
    "    Obtain distance to center coordinates for the entire x,y array passed.\n",
    "    '''\n",
    "\n",
    "    # delta_x, delta_y = abs(x - center[0]), abs(y - center[1])\n",
    "    delta_x, delta_y = (x - center[0]), (y - center[1])\n",
    "    dist = np.sqrt(delta_x ** 2 + delta_y ** 2)\n",
    "\n",
    "    return delta_x, delta_y, dist\n",
    "\n",
    "\n",
    "def get_outer_shell(center, x, y):\n",
    "    '''\n",
    "    Selects those stars located in an 'outer shell' of the points cloud,\n",
    "    according to a given accuracy (ie: the 'delta_angle' of the slices the\n",
    "    circle is divided in).\n",
    "    '''\n",
    "\n",
    "    delta_x, delta_y, dist = dist_2_cent(x, y, center)\n",
    "\n",
    "    # Obtain correct angle with positive x axis for each point.\n",
    "    angles = []\n",
    "    for dx, dy in zip(*[delta_x, delta_y]):\n",
    "        ang = np.rad2deg(np.arctan(abs(dx / dy)))\n",
    "        if dx > 0. and dy > 0.:\n",
    "            angles.append(ang)\n",
    "        elif dx < 0. and dy > 0.:\n",
    "            angles.append(180. - ang)\n",
    "        elif dx < 0. and dy < 0.:\n",
    "            angles.append(270. - ang)\n",
    "        elif dx > 0. and dy < 0.:\n",
    "            angles.append(360. - ang)\n",
    "\n",
    "    # Get indexes of angles from min to max value.\n",
    "    min_max_ind = np.argsort(angles)\n",
    "\n",
    "    # Determine sliced circumference. 'delta_angle' sets the number of slices.\n",
    "    delta_angle = 1.\n",
    "    circle_slices = np.arange(delta_angle, 361., delta_angle)\n",
    "\n",
    "    # Fill outer shell with as many empty lists as slices.\n",
    "    outer_shell = [[] for _ in range(len(circle_slices))]\n",
    "    # Initialize first angle value (0\\degrees) and index of stars in list\n",
    "    # ordered from min to max distance value to center.\n",
    "    ang_slice_prev, j = 0., 0\n",
    "    # For each slice.\n",
    "    for k, ang_slice in enumerate(circle_slices):\n",
    "        # Initialize previous maximum distance and counter of stars that have\n",
    "        # been processed 'p'.\n",
    "        dist_old, p = 0., 0\n",
    "        # For each star in the list, except those already processed (ie: with\n",
    "        # an angle smaller than 'ang_slice_prev')\n",
    "        for i in min_max_ind[j:]:\n",
    "            # If the angle is within the slice.\n",
    "            if ang_slice_prev <= angles[i] < ang_slice:\n",
    "                # Increase the index that stores the number of stars processed.\n",
    "                p += 1\n",
    "                # If the distance to the center is greater than the previous\n",
    "                # one found (if any).\n",
    "                if dist[i] > dist_old:\n",
    "                    # Store coordinates of new star farthest away from center\n",
    "                    # in this slice.\n",
    "                    outer_shell[k] = [x[i], y[i]]\n",
    "                    # Re-assign previous max distance value.\n",
    "                    dist_old = dist[i]\n",
    "            # If the angle value is greater than the max slice value.\n",
    "            elif angles[i] >= ang_slice:\n",
    "                # Increase index of last star processed and break out of\n",
    "                # stars loop.\n",
    "                j += p\n",
    "                break\n",
    "\n",
    "        # Re-assign minimum slice angle value.\n",
    "        ang_slice_prev = ang_slice\n",
    "\n",
    "    # Remove empty lists from array (ie: slices with no stars in it).\n",
    "    outer_shell = np.asarray([x for x in outer_shell if x != []])\n",
    "\n",
    "    return outer_shell\n",
    "\n",
    "\n",
    "def random_points():\n",
    "    mu, sigma = np.random.uniform(-10, 10), np.random.uniform(0., 10)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def run_mvee(array_2d, plotter=True):\n",
    "\n",
    "    # # Generate some random points to test\n",
    "    # N = 2000\n",
    "    # mux, sigmax = random_points()\n",
    "    # muy, sigmay = random_points()\n",
    "    # x = np.random.normal(mux, sigmax, N)\n",
    "    # y = np.random.normal(muy, sigmay, N)\n",
    "    # center = [mux, muy]\n",
    "    # points = get_outer_shell(center, x, y)\n",
    "\n",
    "    # center the 2d array at 0.0 i.e., mean centering\n",
    "    data_array = array_2d - np.mean(array_2d, axis=0)\n",
    "\n",
    "    # Extract x and y coordinates from mean centered array\n",
    "    x, y = data_array[:, 0], data_array[:, 1]\n",
    "\n",
    "    # Use the centroid of data as the initial center for get_outer_shell\n",
    "    center = [np.mean(x), np.mean(y)]\n",
    "    points = get_outer_shell(center, x, y)\n",
    "\n",
    "    # Run the MVEE algorithm\n",
    "    A, centroid = mvee(points)\n",
    "\n",
    "    # Extract ellipse parameters\n",
    "    U, D, V = la.svd(A)\n",
    "    rx, ry = 1./np.sqrt(D)\n",
    "    dx, dy = 2 * rx, 2 * ry\n",
    "    a, b = max(dx, dy), min(dx, dy)\n",
    "    mvee_anis = a/b\n",
    "    alpha = np.rad2deg(np.arccos(V[0][1]))\n",
    "\n",
    "    if plotter:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(x, y, s=10, zorder=4)\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=75, c='r', zorder=3)\n",
    "        ax.scatter(*centroid, s=70, c='g')\n",
    "        ax.scatter(center[0], center[1], c='k', s=70)\n",
    "        ellipse = Ellipse(xy=centroid, width=a, height=b, edgecolor='k',\n",
    "                          angle=alpha, fc='None', lw=2)\n",
    "        plt.savefig('MVEE Anisotropy Plot.tiff', dpi=300, bbox_inches='tight')\n",
    "        ax.add_patch(ellipse)\n",
    "        plt.show(block=False)\n",
    "    return mvee_anis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_anisotropy(array_2d, type, plotter=True):\n",
    "\n",
    "    # Initialize variables\n",
    "    global_anis = None\n",
    "    local_anis = []\n",
    "    arithmetic_anis = None\n",
    "    harmonic_anis = None\n",
    "    geometric_anis = None\n",
    "\n",
    "    # Perform mean centering to 0 on array\n",
    "    temp = array_2d.copy()\n",
    "    data = temp - np.mean(temp, axis=0)\n",
    "    center = np.mean(data, axis=0)\n",
    "\n",
    "    # Estimate the PDF using Gaussian Kernel Density Estimation\n",
    "    kde = gaussian_kde(data.T)\n",
    "    x_grid, y_grid = np.mgrid[data[:,0].min():data[:,0].max():100j, data[:,1].min():data[:,1].max():100j]\n",
    "    pdf_values = kde(np.vstack([x_grid.ravel(), y_grid.ravel()]))\n",
    "\n",
    "    # Reshape for contour plot\n",
    "    pdf_values = pdf_values.reshape(x_grid.shape)\n",
    "\n",
    "    # Make the plot\n",
    "    if plotter:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "\n",
    "        # Main scatter plot\n",
    "        ax_main = plt.subplot(gs[1:4, 0:3])\n",
    "\n",
    "        if type.lower() == 'local':\n",
    "            level_95 = np.percentile(pdf_values, 95)\n",
    "            levels_below_95 = np.linspace(pdf_values.min(), level_95, num=5, endpoint=False)\n",
    "            linewidths = np.linspace(0.5, 1.5, num=len(levels_below_95)) # make linewidths increasing wrt level\n",
    "            contour_lines = ax_main.contour(x_grid, y_grid, pdf_values, levels=levels_below_95, colors='black', linewidths=linewidths)\n",
    "            contour_lines_95 = ax_main.contour(x_grid, y_grid, pdf_values, levels=[level_95], colors='blue', linestyles='dashed', linewidths=1.75)\n",
    "            ax_main.clabel(contour_lines, inline=True, fontsize=8, fmt='%.2f')\n",
    "            ax_main.clabel(contour_lines_95, inline=True, fontsize=8, fmt='%.2f')\n",
    "\n",
    "        else:\n",
    "            levels = np.linspace(pdf_values.min(), pdf_values.max(), num=6, endpoint=False)\n",
    "            linewidths = np.linspace(0.5, 1.75, num=len(levels))\n",
    "            contour_lines_95 = ax_main.contour(x_grid, y_grid, pdf_values, levels=levels, colors='black', linewidths=linewidths) # for entire pdf, not at 95 percentile like the name suggests. This is because plotting objects cannot be copied\n",
    "            ax_main.clabel(contour_lines_95, inline=True, fontsize=8, fmt='%.2f')\n",
    "            ax_main.plot(data[:, 0], data[:, 1], 'k.', markersize=2)\n",
    "\n",
    "        ax_main.scatter(center[0], center[1], c='r', s=2, zorder=3)\n",
    "        ax_main.set_xlabel('LS 1')\n",
    "        ax_main.set_ylabel('LS 2')\n",
    "\n",
    "        # Marginal distributions\n",
    "        ax_xDist = plt.subplot(gs[0, 0:3], sharex=ax_main)\n",
    "        ax_yDist = plt.subplot(gs[1:4, 3], sharey=ax_main)\n",
    "\n",
    "        # Plotting marginal distributions as KDE's\n",
    "        n_x, bins_x, patches_x = ax_xDist.hist(data[:, 0], bins=knuth_bin_width(data[:, 0]), orientation='vertical', color='black', alpha=0.3)  # Histogram\n",
    "        kde_x = gaussian_kde(data[:, 0])\n",
    "        x_line = np.linspace(bins_x.min(), bins_x.max(), 1000)\n",
    "        ax_xDist.plot(x_line, kde_x(x_line) * np.diff(bins_x).mean() * len(data[:, 0]), color='black')  # KDE curve\n",
    "        ax_xDist.set_ylabel('Frequency')\n",
    "\n",
    "        n_y, bins_y, patches_y = ax_yDist.hist(data[:, 1], bins=knuth_bin_width(data[:, 1]), orientation='horizontal', color='black', alpha=0.3)  # Histogram\n",
    "        kde_y = gaussian_kde(data[:, 1])\n",
    "        y_line = np.linspace(bins_y.min(), bins_y.max(), 1000)\n",
    "        ax_yDist.plot(kde_y(y_line) * np.diff(bins_y).mean() * len(data[:, 1]), y_line, color='black')  # KDE curve\n",
    "        ax_yDist.set_xlabel('Frequency')\n",
    "\n",
    "        # Turn off ticks for marginal distributions\n",
    "        ax_xDist.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax_yDist.yaxis.set_tick_params(labelleft=False)\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    if type.lower() == 'global':\n",
    "        # Compute covariance matrix for the entire dataset\n",
    "        cov = np.cov(data.T)\n",
    "        eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "        order = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "\n",
    "        # Ellipse parameters for the entire dataset\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        overall_ellipse = Ellipse(xy=np.mean(data, axis=0), width=width, height=height, angle=angle, edgecolor='red', fc='None', lw=2)\n",
    "\n",
    "        # Compute anisotropy global\n",
    "        global_anis = width / height\n",
    "\n",
    "    elif type.lower() == 'local':\n",
    "        # Contour level at 95% confidence interval\n",
    "        level = np.percentile(pdf_values, 95)\n",
    "\n",
    "        if plotter:\n",
    "            ax_main.plot(data[:, 0], data[:, 1], 'k.', markersize=2)\n",
    "            # Find and fit ellipses to all such contours, which is indicative of tentative cluster structures within an array if any\n",
    "            for path in contour_lines_95.collections[0].get_paths(): # Grab paths of 95% CI\n",
    "                contains_points = path.contains_points(data)\n",
    "                ax_main.plot(data[contains_points, 0], data[contains_points, 1], 'b.', markersize=2) # Plot data points inside the 95% CI\n",
    "                v = path.vertices\n",
    "                cov = np.cov(v, rowvar=False)\n",
    "                eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "                order = eigenvalues.argsort()[::-1]\n",
    "                eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "                angle = np.arctan2(*eigenvectors[:, 0][::-1])\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ellipse = Ellipse(xy=np.mean(v, axis=0), width=width, height=height, angle=np.degrees(angle), edgecolor='red', fc='None')\n",
    "\n",
    "                # Add each ellipse to the plot\n",
    "                if plotter:\n",
    "                    #ax.add_patch(ellipse)\n",
    "                    ax_main.add_patch(ellipse)\n",
    "\n",
    "                # Compute local anisotropies with respect to bi or multimodal pdf's and save\n",
    "                local_anis.append(width / height)\n",
    "        else:\n",
    "            # Find and fit ellipses to all such contours, which is indicative of tentative cluster structures within an array if any\n",
    "            # for path in plt.contour(x_grid, y_grid, pdf_values, levels=[level], colors='r').collections[0].get_paths():\n",
    "            #     v = path.vertices\n",
    "            contours = measure.find_contours(pdf_values, level=level)\n",
    "            for contour in contours:\n",
    "                v = contour\n",
    "                cov = np.cov(v, rowvar=False)\n",
    "                eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "                order = eigenvalues.argsort()[::-1]\n",
    "                eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "                angle = np.arctan2(*eigenvectors[:, 0][::-1])\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ellipse = Ellipse(xy=np.mean(v, axis=0), width=width, height=height, angle=np.degrees(angle), edgecolor='red', fc='None')\n",
    "\n",
    "                # Compute local anisotropies with respect to bi or multimodal pdf's and save\n",
    "                local_anis.append(width / height)\n",
    "\n",
    "    # Compute the harmonic and geometric means of the local anisotropy\n",
    "    arithmetic_anis = np.mean(local_anis) if local_anis else None\n",
    "    harmonic_anis = scipy.stats.hmean(local_anis) if local_anis else None\n",
    "    geometric_anis = scipy.stats.gmean(local_anis) if local_anis else None\n",
    "\n",
    "    if  plotter and type.lower() == 'global':\n",
    "        #ax.add_patch(overall_ellipse)\n",
    "        ax_main.add_patch(overall_ellipse)\n",
    "        black_line = mlines.Line2D([], [], color='black', label='PDF')\n",
    "        red_line = mlines.Line2D([], [], color='red', label='Global Anisotropy')\n",
    "        data_pt = mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=1, label='Sample')\n",
    "\n",
    "        # Add the legend to the plot\n",
    "        ax_main.legend(handles=[data_pt, black_line, red_line], loc='best')\n",
    "        plt.savefig('Global Anisotropy.tiff', dpi=300, bbox_inches='tight')\n",
    "        plt.show(block=False)\n",
    "\n",
    "    if plotter and type.lower() == 'local':\n",
    "        # Create proxy artists for the legend\n",
    "        black_line = mlines.Line2D([], [], color='black', label='PDF')\n",
    "        blue_line = mlines.Line2D([], [], color='blue', linestyle='dashed', label='95% CI')\n",
    "        red_line = mlines.Line2D([], [], color='red', label='Local Anisotropy')\n",
    "        data_pt = mlines.Line2D([], [], color='black', marker='o', linestyle='None', markersize=1, label='Sample')\n",
    "\n",
    "        # Add the legend to the plot\n",
    "        ax_main.legend(handles=[data_pt, black_line, blue_line, red_line], loc='best')\n",
    "        plt.savefig('Local Anisotropy.tiff', dpi=300, bbox_inches='tight')\n",
    "        plt.show(block=False)\n",
    "    return global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def weighted_percentile(data, weights, perc): # finds weighted percentiles\n",
    "    if len(data) != len(weights):\n",
    "        raise ValueError(\"Data and weights must be the same length\")\n",
    "\n",
    "    # Assert non-negative weights\n",
    "    if np.any(weights < 0):\n",
    "        raise ValueError(\"Weights must be non-negative\")\n",
    "\n",
    "    # Sort dta and weigh\n",
    "    ix = np.argsort(data)\n",
    "    data_sorted = np.array(data)[ix]\n",
    "    weights_sorted = np.array(weights)[ix]\n",
    "    # Find CDF of weights\n",
    "    cdf = np.cumsum(weights_sorted) / np.sum(weights_sorted)\n",
    "    # Bin centering for cdf to 0\n",
    "    cdf = np.insert(cdf, 0, 0)\n",
    "    cdf = (cdf[:-1] + cdf[1:]) / 2\n",
    "    return np.interp(perc, cdf, data_sorted)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def knuth_bin_width(data):\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of bins using Knuth's Rule.\n",
    "\n",
    "    Parameters:\n",
    "    data (array-like): The input data for which you want to create a histogram.\n",
    "\n",
    "    Returns:\n",
    "    int: The recommended number of bins.\n",
    "    \"\"\"\n",
    "    N_data = len(data)\n",
    "    if N_data <= 0:\n",
    "        raise ValueError(\"Data array must have at least one element\")\n",
    "\n",
    "    # Calculate the number of bins using Knuth's Rule formula\n",
    "    optimal_bin = 1 + np.log2(N_data) + np.log2(1 + np.sqrt(N_data))\n",
    "\n",
    "    # Round N to the nearest integer\n",
    "    optimal_bin = int(np.round(optimal_bin))\n",
    "    return optimal_bin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def histogram_bounds(ax_or_plt, optimal_bin, values, weights, color, max_freq_override=None): # finds uncertainty bounds p10, p50, p90\n",
    "    hist_data, bin_edges = np.histogram(values, bins=optimal_bin)\n",
    "    max_freq = max(hist_data)\n",
    "    max_freq += max_freq * 0.05\n",
    "\n",
    "    if max_freq_override is not None:\n",
    "        max_freq = max_freq_override\n",
    "\n",
    "    p10 = weighted_percentile(values, weights, 0.1)\n",
    "    p50 = np.average(values, weights=weights)\n",
    "    p90 = weighted_percentile(values, weights, 0.9)\n",
    "    plot_function = ax_or_plt if hasattr(ax_or_plt, 'plot') else plt\n",
    "\n",
    "    plot_function.plot([p10, p10], [0.0, max_freq], color=color, linestyle='dashed', label='P10')\n",
    "    plot_function.plot([p50, p50], [0.0, max_freq], color=color, label='P50')\n",
    "    plot_function.plot([p90, p90], [0.0, max_freq], color=color, linestyle='dotted', label='P90')\n",
    "    # plot_function.legend()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modified_raw_stress(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Compute raw stress between two sets of embeddings using Euclidean distance on a point by point basis\n",
    "\n",
    "    Args:\n",
    "        embeddings1 (numpy.ndarray): Array of embeddings from the first autoencoder.\n",
    "        embeddings2 (numpy.ndarray): Array of embeddings from the second autoencoder.\n",
    "\n",
    "    Returns:\n",
    "        float: Stress value indicating dissimilarity between the embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings have the same shape\n",
    "    if embeddings1.shape != embeddings2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape\")\n",
    "\n",
    "    squared_diff = np.sum((embeddings1 - embeddings2)**2, axis=1)\n",
    "\n",
    "    stress = np.mean(np.sqrt(squared_diff))\n",
    "    return stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modified_norm_stress(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Compute normalized adjusted stress between two sets of embeddings on a structural basis\n",
    "\n",
    "    Args:\n",
    "        embeddings1 (numpy.ndarray): Array of embeddings from the first autoencoder.\n",
    "        embeddings2 (numpy.ndarray): Array of embeddings from the second autoencoder.\n",
    "\n",
    "    Returns:\n",
    "        float: Adjusted MDS stress indicating dissimilarity between the embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings have the same shape\n",
    "    if embeddings1.shape != embeddings2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape\")\n",
    "\n",
    "    dists1 = squareform(pdist(embeddings1, 'euclidean'))\n",
    "    dists2 = squareform(pdist(embeddings2, 'euclidean'))\n",
    "\n",
    "    numerator = np.sum((dists1 - dists2) ** 2)\n",
    "    denominator = np.sum(dists1 ** 2)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    stress = np.sqrt(numerator / denominator)\n",
    "    return stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjusted_stress(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Compute normalized adjusted stress between two sets of embeddings on a structural basis\n",
    "\n",
    "    Args:\n",
    "        embeddings1 (numpy.ndarray): Array of embeddings from the first autoencoder.\n",
    "        embeddings2 (numpy.ndarray): Array of embeddings from the second autoencoder.\n",
    "\n",
    "    Returns:\n",
    "        float: Adjusted MDS stress indicating dissimilarity between the embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings have the same shape\n",
    "    if embeddings1.shape != embeddings2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape\")\n",
    "\n",
    "    dists1 = squareform(pdist(embeddings1, 'euclidean'))\n",
    "    dists2 = squareform(pdist(embeddings2, 'euclidean'))\n",
    "\n",
    "    numerator = np.sum((dists1 - dists2) ** 2)\n",
    "    denominator = np.sum(dists1 * dists2)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    stress = np.sqrt(numerator / denominator)\n",
    "    return stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "target = load_wine().target\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 10000\n",
    "batch_size = 16\n",
    "dataset = CustomDataset(torch.tensor(df_scaled, dtype=torch.float32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dim = df.shape[1]\n",
    "encoding_dim = 2\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_seed = 0 # starting sequence for random seed generator\n",
    "step_size = 10   # Change this to increase seeds every x units as needed\n",
    "random.seed(initial_seed)\n",
    "num_seeds = 1000 # nos of realizations needed\n",
    "seed_values = [initial_seed + (step_size * i) for i in range(num_seeds)]\n",
    "cluster_changes = np.zeros_like(target)  # To track changes in cluster assignments\n",
    "n_clusters = len(np.unique(target))  # nos of clusters in target if classification/ clustering problem\n",
    "change_percentages = []  # List to store percentage of changed clusters for each realization\n",
    "statistics_array = np.empty((num_seeds, 7))\n",
    "pvalues_array = np.empty((num_seeds, 3))\n",
    "convexhull_array = np.empty((num_seeds, 3))\n",
    "anisotropy_array = np.empty((num_seeds, 5))\n",
    "convex_hull_vertices = []\n",
    "all_loss_curves = []\n",
    "local_anisotropies = []\n",
    "hdf5_file = 'embedding_realizations.h5' # store each latent space for all AE realizations here\n",
    "\n",
    "# Make the HDF5 file\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    pass  # Just to create the file\n",
    "\n",
    "# Make 3x3 subplot for 6 randomly selected AE realizations visualizations\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "random.seed(initial_seed)\n",
    "visualization_indices = random.sample(range(num_seeds), 6)\n",
    "\n",
    "for idx, seed in enumerate(seed_values):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "\n",
    "    model = Autoencoder(input_dim, encoding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    epoch_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        #for data in dataloader:\n",
    "        for data, indices in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_losses.append(total_loss / len(dataloader))\n",
    "    all_loss_curves.append(epoch_losses)\n",
    "\n",
    "################################################ SELF NOTE #############################################################################################################\n",
    "#     # Clustering in latent space\n",
    "#     Here's a breakdown of how it works:\n",
    "#\n",
    "# 1. **Initialization**:\n",
    "#    - `cluster_changes` is initialized as a zero array with the same shape as the `target` (truth labels). This means initially, all data points have 0 changes.\n",
    "#    - `predicted_clusters` is computed for each realization after clustering in the latent space.\n",
    "#\n",
    "# 2. **Matching clusters with truth labels**:\n",
    "#    - A cost matrix is computed to find the best correspondence between predicted clusters and the true labels.\n",
    "#    - The Hungarian algorithm (`linear_sum_assignment`) is used to determine the optimal assignment.\n",
    "#    - After the best assignment is determined, `predicted_clusters` labels are rearranged to match the true labels as closely as possible.\n",
    "#\n",
    "# 3. **Tracking changes**:\n",
    "#    - For each data point, if its assigned cluster (`predicted_clusters`) doesn't match its true label (`target`), its corresponding value in `cluster_changes` is incremented by 1.\n",
    "#    - This process is repeated for all realizations.\n",
    "########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     encoded_data = model.encoder(dataset).numpy()\n",
    "\n",
    "    # Reorder the embeddings gotten based on the originlal data index\n",
    "    with torch.no_grad():\n",
    "        encoded_data, original_indices = zip(*[(model.encoder(d[None, ...]), idx) for d, idx in dataset])\n",
    "        # Reordering step here\n",
    "        encoded_data = torch.cat(encoded_data, dim=0).numpy()\n",
    "        order = np.argsort(original_indices)\n",
    "        encoded_data = encoded_data[order]\n",
    "\n",
    "        # Save each latent space embeddings to an HDF5 file\n",
    "        with h5py.File(hdf5_file, 'a') as f:\n",
    "            f.create_dataset(f'embedding_realizations_{idx}', data=encoded_data)\n",
    "\n",
    "\n",
    "################################################ SELF NOTE #############################################################################################################\n",
    "# The CustomDataset class returns both the data and its index. When shuffled in the DataLoader, the indices will follow the same shuffling order as the data.\n",
    "# During training, you get both data and indices from the DataLoader. However, the indices are not used in training; they're only needed for evaluation.\n",
    "\n",
    "    # When evaluating, you need to reorder the embeddings according to the original indices. This is done by first getting the embeddings and their original indices, and then using np.argsort to reorder the embeddings to match the original data order.\n",
    "\n",
    "    # This approach ensures that, despite shuffling during training, you can align the embeddings from different autoencoders to their corresponding original data points. This alignment is crucial for comparing embeddings across different training runs with different seeds.\n",
    "########################################################################################################################################################################\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10).fit(encoded_data)\n",
    "    predicted_clusters = kmeans.labels_\n",
    "\n",
    "    # Matching predicted clusters to true clusters\n",
    "    cost_matrix = -np.array([[np.sum((predicted_clusters[true_indices] == pred_cluster))\n",
    "                              for pred_cluster in range(n_clusters)]\n",
    "                             for true_indices in [np.where(target == i)[0] for i in range(n_clusters)]])\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    for i in range(n_clusters):\n",
    "        predicted_clusters[predicted_clusters == col_ind[i]] = -(i+10)  # Temporarily set to negative values\n",
    "    for i in range(n_clusters):\n",
    "        predicted_clusters[predicted_clusters == -(i+10)] = i\n",
    "\n",
    "    changes = (predicted_clusters != target)\n",
    "    change_percentages.append(100 * np.sum(changes) / len(target))\n",
    "    cluster_changes += changes\n",
    "\n",
    "    # Compute the chaos, global, and spatial statistics for each of the latent spaces for each AE realizations\n",
    "    # Chaos Statistics\n",
    "    array_flat = encoded_data.flatten()\n",
    "    values, counts = np.unique(array_flat, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy_values = entropy(probabilities, base=2) # s1\n",
    "\n",
    "    # Global Statistics\n",
    "    mean_global = np.mean(encoded_data) # s2\n",
    "    variance_global = np.var(encoded_data) # s3\n",
    "    norm_fro = np.linalg.norm(encoded_data, 'fro') # s4\n",
    "    pearson, coef_pval = pearsonr(encoded_data[:,0], encoded_data[:,1]) # s5\n",
    "    spearman, rank_pval = spearmanr(encoded_data[:,0], encoded_data[:,1]) # s6\n",
    "\n",
    "    # Spatial Statistics\n",
    "    weights = libpysal.weights.lat2W(encoded_data.shape[0],encoded_data.shape[1], rook=False, id_type=\"float\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    mi = Moran(encoded_data, weights) # two_tailed=False for one tailed Moran, emeasure of overall spatial autocorrelation -1: perfect clustering of dissimilar values/objects (perfect dispersion), 0: perfect randomness, 1: perfect clustering of similar values/objects.\n",
    "    moran = mi.I # s7\n",
    "    moran_pval = mi.p_norm\n",
    "\n",
    "    # Store the computed statistics in the array for this iteration\n",
    "    statistics_array[idx, :] = [entropy_values, mean_global, variance_global, norm_fro, pearson, spearman, moran]\n",
    "    pvalues_array[idx, :] = [coef_pval, rank_pval, moran_pval]\n",
    "\n",
    "    # Save statistics and pvalues array to numpy files\n",
    "    np.save('statistics_array.npy', statistics_array)\n",
    "    np.save('pvalues_array.npy', pvalues_array)\n",
    "\n",
    "    # Compute convex hull and associated attributes\n",
    "    my_points = encoded_data.copy()\n",
    "    hull = ConvexHull(my_points)\n",
    "    vertices_idx = hull.vertices\n",
    "    vertices = my_points[hull.vertices]\n",
    "    polygon = Polygon(vertices)\n",
    "\n",
    "    # Store computed variables needed from convex hull polygon i.e., the number of vertices, area, and volume for each AE realization\n",
    "    convexhull_array[idx, :] = [len(vertices), hull.area, hull.volume]\n",
    "    convex_hull_vertices.append(vertices_idx)\n",
    "\n",
    "    # Save convex hull attributes to numpy files\n",
    "    np.save('convexhull_array.npy', convexhull_array)\n",
    "\n",
    "    # Different types of anisotropy computations\n",
    "    mvee_anis= run_mvee(array_2d=encoded_data, plotter=False)\n",
    "    global_anis, _, _, _, _ = compute_anisotropy(array_2d=encoded_data, type='global', plotter=False)\n",
    "    _, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='local', plotter=False)\n",
    "    local_anisotropies.append(np.array(local_anis))\n",
    "    anisotropy_array[idx, :] = [mvee_anis, global_anis, arithmetic_anis, harmonic_anis, geometric_anis]\n",
    "\n",
    "    # Save and store anisotropy attributes to numpy files or whatever format as needed\n",
    "    np.save('anisotropy_array.npy', anisotropy_array)\n",
    "\n",
    "    # Check if the current realization index is in the list of visualization_indices\n",
    "    if idx in visualization_indices:\n",
    "        # Visualization of latent space in 3x3 grid\n",
    "        scatter = axs[visualization_indices.index(idx)].scatter(encoded_data[:, 0], encoded_data[:, 1], c=predicted_clusters, cmap='viridis', marker='o', s=30,\n",
    "                      alpha=0.7, edgecolor='black', zorder=2)\n",
    "        axs[visualization_indices.index(idx)].set_title(f\"Realization {idx + 1}, Seed: ${seed}$\")\n",
    "\n",
    "        for simplex in hull.simplices:\n",
    "            axs[visualization_indices.index(idx)].plot(my_points[simplex, 0], my_points[simplex, 1], 'r--', zorder=1)\n",
    "            #axs[visualization_indices.index(idx)].fill(my_points[hull.vertices, 0], my_points[hull.vertices, 1], c='yellow', alpha=0.01)\n",
    "\n",
    "        # Adding Y-axis labels to the first column\n",
    "        for ax in range(2):\n",
    "            axs[ax * 3].set_ylabel(\"LS 2\")\n",
    "\n",
    "        # Adding X-axis label to the bottom row\n",
    "        for ax in axs[3:]:\n",
    "            ax.set_xlabel(\"LS 1\")\n",
    "\n",
    "        # Assuming the number of clusters is defined\n",
    "        n_clusters = len(np.unique(target))\n",
    "        viridis = ListedColormap(plt.cm.viridis(np.linspace(0, 1, n_clusters)))\n",
    "\n",
    "        # Then, for each cluster, create a Line2D object for the legend\n",
    "        legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'Cluster {i+1}',\n",
    "                                  markersize=10, markerfacecolor=viridis.colors[i]) for i in range(n_clusters)]\n",
    "\n",
    "fig.legend(handles=legend_elements, loc='lower center', bbox_to_anchor=(0.5, -0.10), ncol=n_clusters)\n",
    "plt.tight_layout()\n",
    "plt.savefig('LS Visualization for Select AE Realizations with Convex Hull Line.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Recall in each realization, we do the following:**\n",
    "\n",
    "1. Train an autoencoder, and then applying k-means to the latent space to identify clusters.\n",
    "2. These clusters are then matched to the true labels to minimize the discrepancies between predicted and true cluster assignments.\n",
    "3. After matching, for each data point, if the matched predicted cluster is different from its true label, it's considered a \"change\", and the corresponding entry in `cluster_changes` is incremented.\n",
    "\n",
    "The `cluster_changes` array accumulates these \"changes\" for each data point over all realizations. After all realizations are done, the value for a data point in `cluster_changes` represents how many times (out of all realizations) the predicted cluster for that data point didn't match its true label. This becomes a tell-tale sign of stability in the latent feature space.\n",
    "\n",
    "The introduction of stochastic uncertainty through changing the random seed is a way to account for and analyze the variability in model training and performance. By training a model multiple times with different seeds, you can assess the robustness of the model and gain insights into how sensitive it is to variations in initialization and training data presentation. It helps in understanding the model's generalization and can be useful for hyperparameter tuning and model evaluation.\n",
    "\n",
    "\n",
    "**Advantages of Minibatch Training for Small Datasets:**\n",
    "\n",
    "1. Regularization: Minibatch training introduces noise into the training process, which can act as a form of regularization. This can help prevent overfitting, even on small datasets.\n",
    "2. Efficiency: Minibatch training allows you to take advantage of parallelism, such as using GPUs, which can significantly speed up training even for small datasets.\n",
    "3. Convergence: Minibatch training can lead to faster convergence, which means your model reaches a good solution more quickly.\n",
    "4. Memory Efficiency: Training on the full dataset might not fit into memory, especially when the dataset is very large. Minibatch training allows you to work with smaller portions of the data at a time.\n",
    "\n",
    "**Considerations for Minibatch Training on Small Datasets:**\n",
    "\n",
    "1. Batch Size: When working with a small dataset, you should carefully choose the batch size. Very small batch sizes can introduce high variance in gradient updates, while very large batch sizes may not provide the regularization benefits of minibatch training. You may need to experiment to find the optimal batch size for your specific dataset and model.\n",
    "2. Learning Rate: Smaller batch sizes may require smaller learning rates to prevent overshooting during optimization. Again, hyperparameter tuning may be necessary.\n",
    "3. Validation Set: With a small dataset, it's crucial to have a validation set to monitor model performance. Overfitting can still occur, especially if your model has many parameters.\n",
    "4. Data Augmentation: Data augmentation techniques (e.g., flipping, rotating, cropping) can help artificially increase the effective size of your dataset, making minibatch training more effective.\n",
    "5. Transfer Learning: If your small dataset is similar to a larger dataset, you can consider using pre-trained models and fine-tuning them on your dataset.\n",
    "6. In summary, while minibatch training can be beneficial for small datasets, it requires careful consideration of batch size, learning rate, and other hyperparameters. Additionally, techniques like data augmentation and transfer learning can be used to make the most of small datasets. Ultimately, the choice of whether to use minibatch training or not depends on the specific problem and constraints you are dealing with.\n",
    "\n",
    "\n",
    "Idea of using the convex hull vertices to study the variability of inferences in different autoencoder realizations is quite innovative and can be a valuable approach. Here's why this could be beneficial and how you might go about it:\n",
    "\n",
    "\n",
    "**Advantages of Using Convex Hull Vertices:**\n",
    "1. Stability Analysis: By examining how the vertices of the convex hull change with different random seeds, you can assess the stability and robustness of the autoencoder. Consistent vertices across different realizations suggest that the autoencoder is reliably capturing the same underlying structure in the data.\n",
    "2. Identifying Anchor Points: Convex hull vertices can serve as \"anchor points\" in your latent space, providing a way to understand the extremities or boundaries of the representations learned by the autoencoder.\n",
    "3. Quantitative Analysis: Changes in the vertices can be quantified, allowing for a more systematic analysis of variability. Metrics like the distance between corresponding vertices in different realizations, or the area/volume of the convex hulls, can provide insights into the variability induced by different seeds.\n",
    "\n",
    "\n",
    "**Implementing the Analysis:**\n",
    "1. Compute Convex Hull: For each autoencoder realization, use the encoded latent space representations to compute the convex hull. Tools like scipy.spatial.ConvexHull in Python can be useful for this.\n",
    "2. Compare Vertices: Analyze how the vertices of these convex hulls vary across different realizations. You might consider:\n",
    "    * The position of vertices.\n",
    "    * The number of vertices.\n",
    "    * Geometric properties like the area or volume enclosed by the convex hull.\n",
    "3. Statistical Analysis: Perform statistical tests or visualizations to understand the degree of variability. For instance, plotting the convex hulls from different realizations can visually show the differences.\n",
    "4. Correlation with Seeds: Investigate if there's any pattern or correlation between the changes in the convex hull and the random seeds used. This might reveal how sensitive the autoencoder's latent space is to initialization.\n",
    "\n",
    "**Considerations:**\n",
    "1. Dimensionality: If your latent space is high-dimensional, the interpretation of convex hulls can become complex. Consider dimensionality reduction techniques for visualization and analysis.\n",
    "2. Comparability: Ensure that the method used for comparing vertices is consistent and meaningful across different realizations.\n",
    "\n",
    "\n",
    "In summary, using the convex hull vertices to analyze the impact of different random seeds on the latent space of an autoencoder is a promising approach. It can provide insights into the stability and variability of the autoencoder's learned representations, aiding in the understanding of how different initializations affect the model's inferences.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize training loss for select AE realizations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_val = np.min(all_loss_curves)\n",
    "print('The minimum loss for all AE realizations is: ',loss_val)\n",
    "\n",
    "# Generate alpha values within the range [0, 1]\n",
    "alphas = [i/num_seeds for i in range(1, num_seeds + 1)]\n",
    "\n",
    "# Visualize loss functions for select realizations\n",
    "for idx, losses in enumerate(all_loss_curves):\n",
    "    if idx in visualization_indices:\n",
    "        plt.plot(losses, label=f'Realization #{idx+1}', alpha=alphas[idx])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Epochs for Select AE Realizations')\n",
    "plt.legend()\n",
    "plt.savefig('Training loss over select realizations.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the percentage changes in anchors found via convex hull\n",
    "change_anchors = percentage_change(convex_hull_vertices, data_type='list')\n",
    "\n",
    "# Compute the percentage changes in anisotropies found\n",
    "change_mvee_anis = percentage_change(anisotropy_array[:,0], data_type='numpy')\n",
    "change_global_anis = percentage_change(anisotropy_array[:,1], data_type='numpy')\n",
    "change_arith_anis = percentage_change(anisotropy_array[:,2], data_type='numpy')\n",
    "change_har_anis = percentage_change(anisotropy_array[:,3], data_type='numpy')\n",
    "change_geo_anis = percentage_change(anisotropy_array[:,4], data_type='numpy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimal_bin = knuth_bin_width(change_percentages)\n",
    "plt.hist(change_percentages, bins=optimal_bin, color='darkorange', edgecolor='black')\n",
    "histogram_bounds(ax_or_plt=plt, optimal_bin=optimal_bin, values=change_percentages, weights=np.ones(len(change_percentages)), color='red')\n",
    "plt.xlabel('Changes in Reassigned Points (%)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Instability in Cluster Assignments')\n",
    "plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend(loc='lower center', bbox_to_anchor=(0.50, -0.3), ncol=3)\n",
    "plt.savefig('Instability in Cluster Assignemnts.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Find optimal bins for each stats\n",
    "optimal_bin1 = knuth_bin_width(change_anchors)\n",
    "optimal_bin2 = knuth_bin_width(change_global_anis)\n",
    "optimal_bin3 = knuth_bin_width(change_har_anis)\n",
    "\n",
    "# Compute histogram data\n",
    "hist_data_1, _ = np.histogram(change_anchors, bins=optimal_bin1)\n",
    "hist_data_2, _ = np.histogram(change_global_anis, bins=optimal_bin2)\n",
    "hist_data_3, _ = np.histogram(change_har_anis, bins=optimal_bin3)\n",
    "\n",
    "# Obtain the maximum frequency from all histogram data since the sample size is same for each metric i.e. 1000 AE realizations\n",
    "overall_max_freq = max(max(hist_data_1), max(hist_data_2), max(hist_data_3))\n",
    "overall_max_freq += overall_max_freq * 0.05\n",
    "\n",
    "# Visualize percentage changes for shape-based statistics\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4)) #-o\n",
    "axs[0].hist(change_anchors, bins=optimal_bin1, color='darkorange', edgecolor='black')\n",
    "histogram_bounds(ax_or_plt=axs[0], optimal_bin=optimal_bin1, values=change_anchors,weights=np.ones(len(change_anchors)), color='red', max_freq_override=overall_max_freq)\n",
    "axs[0].set_xlabel('Changes in Reassigned Anchors (%)')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "axs[0].set_title('Instability in Anchor Points')\n",
    "axs[0].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "axs[1].hist(change_global_anis, bins=optimal_bin2, color='darkorange', edgecolor='black')\n",
    "histogram_bounds(ax_or_plt=axs[1], optimal_bin=optimal_bin2, values=change_global_anis,weights=np.ones(len(change_global_anis)),color='red', max_freq_override=overall_max_freq)\n",
    "axs[1].set_xlabel('Sequential Changes in Global Anisotropy (%)')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "axs[1].set_title('Instability in Global Anisotropy')\n",
    "axs[1].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "axs[2].hist(change_har_anis, bins=optimal_bin3, color='darkorange', edgecolor='black') # based off F1 score computation INSPO!\n",
    "histogram_bounds(ax_or_plt=axs[2], optimal_bin=optimal_bin, values=change_har_anis,weights=np.ones(len(change_har_anis)),color='red', max_freq_override=overall_max_freq)\n",
    "axs[2].set_xlabel('Sequential Changes in Harmonic Anisotropy (%)')\n",
    "axs[2].set_ylabel('Frequency')\n",
    "axs[2].set_title('Instability in Local Anisotropy')\n",
    "axs[2].yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "fig.legend(lines[:3], labels[:3], loc='lower center', bbox_to_anchor=(0.75, -0.0), ncol=3)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.3, right=1.5, top=1.3, wspace=0.25, hspace=0.3)\n",
    "plt.savefig('Instability in Shape-based Statistics.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stress Computations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stress computation for unique pairs only\n",
    "with h5py.File(hdf5_file, 'r') as f:\n",
    "    N = len(f.keys())\n",
    "    # Initialize matrix\n",
    "    adjusted_stress_matrix = np.zeros((N, N))\n",
    "\n",
    "    for i in range(N):\n",
    "        embedding_i = f[f'embedding_realizations_{i}'][:]\n",
    "        for j in range(i + 1, N):  # Starting from i + 1 to avoid redundant computations\n",
    "            embedding_j = f[f'embedding_realizations_{j}'][:]\n",
    "\n",
    "            # Compute adjusted raw_stress\n",
    "            adj_stress = adjusted_stress(embedding_i, embedding_j)\n",
    "            adjusted_stress_matrix[i, j] = adj_stress\n",
    "            adjusted_stress_matrix[j, i] = adj_stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute and Visualize stress measures\n",
    "ordered_dist_mat1, res_order1, res_linkage1 = compute_serial_matrix(adjusted_stress_matrix,\"ward\")\n",
    "\n",
    "# Extract the lower triangular matrix for the sorted stress matrices excluding diagonal and visualize via kde\n",
    "lower_triangle1 = np.tril(ordered_dist_mat1, k=-1) # Adjusted stress\n",
    "flatten_data1 = lower_triangle1[lower_triangle1 != 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8))\n",
    "# Adjusted stress\n",
    "im1 = axs[0].imshow(adjusted_stress_matrix, cmap=\"viridis\")\n",
    "axs[0].set_title(\"Unsorted Adjusted Stress Matrix\", size=20)\n",
    "axs[0].set_xlabel(\"AE Realization Index, $i$\", size=18)\n",
    "axs[0].set_ylabel(\"AE Realization Index, $j$\", size=18)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "# Aesthetics\n",
    "divider = make_axes_locatable(axs[0])\n",
    "cax1 = divider.append_axes(\"bottom\", size=\"5%\", pad=0.85)\n",
    "cbar1 = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.25)\n",
    "cbar1.set_label('Adjusted Stress', size=18)\n",
    "cbar1.ax.tick_params(labelsize=18)\n",
    "\n",
    "# Sorted Adjusted Stress\n",
    "im2 = axs[1].imshow(ordered_dist_mat1, cmap=\"viridis\")\n",
    "axs[1].set_title(\"Sorted Adjusted Stress Matrix\", size=20)\n",
    "axs[1].set_xlabel(\"AE Realization Index, $i$\", size=18)\n",
    "axs[1].set_ylabel(\"AE Realization Index, $j$\", size=18)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "# Aesthetics\n",
    "divider = make_axes_locatable(axs[1])\n",
    "cax2 = divider.append_axes(\"bottom\", size=\"5%\", pad=0.85)\n",
    "cbar2 = fig.colorbar(im2, cax=cax2, orientation='horizontal', pad=0.25)\n",
    "cbar2.set_label('Adjusted Stress', size=18)\n",
    "cbar2.ax.tick_params(labelsize=18)\n",
    "\n",
    "# KDE of lower triangular matrix of adjusted stress exclusing the diagonal\n",
    "sns.kdeplot(flatten_data1, ax=axs[2], color='green', label='Adjusted Stress', linewidth=2)\n",
    "axs[2].set_title(\"Combinatorial Changes for AE Realizations\", size=20)\n",
    "axs[2].set_xlabel('Lower Triangular Matrix', fontsize=18)\n",
    "axs[2].set_ylabel('Density', fontsize=18)\n",
    "axs[2].tick_params(labelsize=18)\n",
    "axs[2].legend(fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig('All Stress Analysis for Real Dataset.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # USE ONLY IN SYNTHETIC DATA CASE\n",
    "#\n",
    "# # Extract the lower triangular matrix for the sorted stress matrices excluding diagonal and visualize via kde\n",
    "# # For correlation = 0\n",
    "# lower_triangle1_0 = np.tril(ordered_dist_mat1_0, k=-1)# Adjusted stress\n",
    "# flatten_data1_0 = lower_triangle1_0[lower_triangle1_0 != 0]\n",
    "#\n",
    "# # For correlation = 0.5\n",
    "# lower_triangle1_1 = np.tril(ordered_dist_mat1_1, k=-1)# Adjusted stress\n",
    "# flatten_data1_1 = lower_triangle1_1[lower_triangle1_1 != 0]\n",
    "#\n",
    "# # For correlation = 1.0\n",
    "# lower_triangle1_2 = np.tril(ordered_dist_mat1_2, k=-1)# Adjusted stress\n",
    "# flatten_data1_2 = lower_triangle1_2[lower_triangle1_2 != 0]\n",
    "#\n",
    "# # Visualize percentage changes for shape-based statistics\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 4))\n",
    "# sns.kdeplot(flatten_data1_0, ax=axs, color='green', label='Correlation = 0.0', linewidth=2)\n",
    "# sns.kdeplot(flatten_data1_1, ax=axs, color='green', label='Correlation = 0.5', linewidth=2)\n",
    "# sns.kdeplot(flatten_data1_2, ax=axs, color='green', label='Correlation = 1.0', linewidth=2)\n",
    "# axs.set_xlabel('Sorted Lower Triangular Stress Matrix', fontsize=14)\n",
    "# axs.set_ylabel('Density', fontsize=16)\n",
    "# axs.set_title('Adjusted Stress', fontsize=18)\n",
    "# axs.tick_params(labelsize=16)\n",
    "# axs.legend()\n",
    "#\n",
    "#\n",
    "# # plt.tight_layout()\n",
    "# #plt.subplots_adjust(left=0.0, bottom=0.3, right=1.5, top=1.3, wspace=0.25, hspace=0.3)\n",
    "# plt.savefig('All Stress Analysis with Different Correlations.tiff', dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Jaccard Similarity Computation on Anchors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The choice of Jaccard similarity for analyzing the vertices of convex hulls across different realizations of an autoencoder is primarily due to its suitability for comparing sets, especially when the elements of these sets are categorical or discrete, like indices in this case. Here are some key reasons why Jaccard similarity is a good fit for this analysis:\n",
    "\n",
    "1. **Measuring Overlap in Sets:** Jaccard similarity is specifically designed to measure the similarity between finite sets. It's calculated as the size of the intersection divided by the size of the union of the sets. In the context of convex hull vertices, it quantifies how many vertices are common between any two realizations relative to the total unique vertices in both.\n",
    "\n",
    "2. **Applicability to Your Analysis:** In your scenario, you're comparing sets of indices (vertices of convex hulls). Jaccard similarity directly addresses this by evaluating how similar these sets are across different realizations, providing a clear and interpretable metric for the stability of your model's latent space representations.\n",
    "\n",
    "3. **Robustness to Size Variations:** The Jaccard index is robust to variations in the size of the sets. It's particularly useful when the number of vertices in the convex hulls might change across realizations, as it normalizes the similarity by the union of the sets.\n",
    "\n",
    "4. **Intuitive and Interpretable:** The Jaccard similarity yields a value between 0 and 1, where 1 indicates identical sets and 0 indicates no common elements. This makes it easy to interpret in terms of similarity or dissimilarity.\n",
    "\n",
    "5. **Widely Used in Comparative Analysis:** Jaccard similarity is a well-established metric in various fields including biology (for species similarity), text analysis (similarity of documents), and machine learning (comparing clusters or sets), making it a reliable choice.\n",
    "\n",
    "Given these properties, Jaccard similarity provides a straightforward and effective way to compare the convex hull vertices across different model realizations and to gauge the stability and consistency of the model's inferences.\n",
    "\n",
    "\n",
    "Jaccard Similarity of 1: A Jaccard similarity score of 1 means that the sets of vertices (convex hulls) being compared are exactly the same. This would mean that two realizations of the autoencoder have produced identical sets of vertices for their convex hulls. This indicates total stability, as the model's latent space representation is consistent across these AE realizations.\n",
    "\n",
    "Jaccard Similarity of 0: Conversely, a Jaccard similarity score of 0 means that there is no overlap between the sets; they are completely different. For my application, this implies total instability, as the sets of vertices (anchor points in the latent space) for the convex hulls in these two realizations do not match at all.\n",
    "Therefore, in terms of evaluating the stability of the model's latent space representations across different AE realizations:\n",
    "\n",
    "High Jaccard scores (close to 1) across most pairs of realizations suggest a high degree of stability. (Hence the use of neffective to determine how many AE realizations is needed to attain stability in the latent space for inference making)\n",
    "\n",
    "Lower Jaccard scores (especially those close to 0) indicate more variability or instability in how the model is representing the data under different initializations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# To compare the sets of vertices from the convex hulls across different realizations, use set operations. The idea is to assess the overlap and differences between the sets of indices for each pair of realizations.\n",
    "\n",
    "comparisons = {}\n",
    "for (i, vertices_i), (j, vertices_j) in combinations(enumerate(convex_hull_vertices), 2):\n",
    "    set_i, set_j = set(vertices_i.tolist()), set(vertices_j.tolist())\n",
    "    intersection = set_i.intersection(set_j)\n",
    "    union = set_i.union(set_j)\n",
    "    jaccard_similarity = len(intersection) / len(union) if union else 1\n",
    "    comparisons[(i, j)] = jaccard_similarity\n",
    "\n",
    "# Compute Jaccard similarity matrix\n",
    "num_realizations = len(convex_hull_vertices)\n",
    "similarity_matrix = np.zeros((num_realizations, num_realizations))\n",
    "\n",
    "for (i, j), similarity in comparisons.items():\n",
    "    similarity_matrix[i, j] = similarity\n",
    "    similarity_matrix[j, i] = similarity  # Mirror the similarity as the matrix is symmetric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute and Visualize Jaccard similarity measures\n",
    "ordered_dist_mat2, res_order2, res_linkage2 = compute_serial_matrix(similarity_matrix,\"ward\")\n",
    "\n",
    "# Extract the lower triangular matrix for the sorted stress matrices excluding diagonal and visualize via kde\n",
    "lower_triangle2 = np.tril(ordered_dist_mat2, k=-1) # Modified raw stress\n",
    "flatten_data2 = lower_triangle2[lower_triangle2 != 0]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(30, 8))\n",
    "# Jaccard Similarity\n",
    "im1 = axs[0].imshow(similarity_matrix, cmap=\"viridis\")\n",
    "axs[0].set_title(\"Unsorted Jaccard Matrix\", size=20)\n",
    "axs[0].set_xlabel(\"AE Realization Index, $i$\", size=18)\n",
    "axs[0].set_ylabel(\"AE Realization Index, $j$\", size=18)\n",
    "axs[0].tick_params(axis='both', which='major', labelsize=18)\n",
    "# Aesthetics\n",
    "divider = make_axes_locatable(axs[0])\n",
    "cax1 = divider.append_axes(\"bottom\", size=\"5%\", pad=0.85)\n",
    "cbar1 = fig.colorbar(im1, cax=cax1, orientation='horizontal', pad=0.25)\n",
    "cbar1.set_label('Jaccard Similarity', size=18)\n",
    "cbar1.ax.tick_params(labelsize=18)\n",
    "\n",
    "# Sorted Jaccard Similarity\n",
    "im2 = axs[1].imshow(ordered_dist_mat2, cmap=\"viridis\")\n",
    "axs[1].set_title(\"Sorted Jaccard Matrix\", size=20)\n",
    "axs[1].set_xlabel(\"AE Realization Index, $i$\", size=18)\n",
    "axs[1].set_ylabel(\"AE Realization Index, $j$\", size=18)\n",
    "axs[1].tick_params(axis='both', which='major', labelsize=18)\n",
    "# Aesthetics\n",
    "divider = make_axes_locatable(axs[1])\n",
    "cax2 = divider.append_axes(\"bottom\", size=\"5%\", pad=0.85)\n",
    "cbar2 = fig.colorbar(im2, cax=cax2, orientation='horizontal', pad=0.25)\n",
    "cbar2.set_label('Jaccard Similarity', size=18)\n",
    "cbar2.ax.tick_params(labelsize=18)\n",
    "\n",
    "# KDE of lower triangular matrix of Jaccard similarity excluding the diagonal\n",
    "sns.kdeplot(flatten_data2, ax=axs[2], bw_method='scott', color='orange', label='Jaccard Similarity', linewidth=2)\n",
    "axs[2].set_title(\"Combinatorial Anchor Changes in AE Realizations\", size=20)\n",
    "axs[2].set_xlabel('Lower Triangular Matrix', fontsize=18)\n",
    "axs[2].set_ylabel('Density', fontsize=18)\n",
    "axs[2].tick_params(labelsize=18)\n",
    "axs[2].legend(fontsize=18)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.2)\n",
    "plt.savefig('All Jaccard Analysis.tiff', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #USE ONLY IN SYNTHETIC DATA CASE\n",
    "\n",
    "# # Extract the lower triangular matrix for the sorted stress matrices excluding diagonal and visualize via kde\n",
    "# # For correlation = 0\n",
    "# lower_triangle2_0 = np.tril(ordered_dist_mat2_0, k=-1) # Jaccard Similarity\n",
    "# flatten_data2_0 = lower_triangle2_0[lower_triangle2_0 != 0]\n",
    "#\n",
    "# # For correlation = 0.5\n",
    "# lower_triangle2_1 = np.tril(ordered_dist_mat2_1, k=-1) # Jaccard Similarity\n",
    "# flatten_data2_1 = lower_triangle2_1[lower_triangle2_1 != 0]\n",
    "#\n",
    "# # For correlation = 1.0\n",
    "# lower_triangle2_2 = np.tril(ordered_dist_mat2_2, k=-1) # Jaccard Similarity\n",
    "# flatten_data2_2 = lower_triangle2_2[lower_triangle2_2 != 0]\n",
    "#\n",
    "# # Visualize percentage changes for shape-based statistics\n",
    "# fig, axs = plt.subplots(1, 1, figsize=(10, 4))\n",
    "# sns.kdeplot(flatten_data2_0, ax=axs, color='orange', label='Correlation = 0.0', linewidth=2)\n",
    "# sns.kdeplot(flatten_data2_1, ax=axs, color='orange', label='Correlation = 0.5', linewidth=2)\n",
    "# sns.kdeplot(flatten_data2_2, ax=axs, color='orange', label='Correlation = 1.0', linewidth=2)\n",
    "# axs.set_xlabel('Sorted Lower Triangular Jaccard Similarity Matrix', fontsize=14)\n",
    "# axs.set_ylabel('Density', fontsize=16)\n",
    "# axs.set_title('Combinatorial Anchor Changes in AE Realizations', fontsize=18)\n",
    "# axs.tick_params(labelsize=16)\n",
    "# axs.legend()\n",
    "#\n",
    "# # plt.tight_layout()\n",
    "# #plt.subplots_adjust(left=0.0, bottom=0.3, right=1.5, top=1.3, wspace=0.25, hspace=0.3)\n",
    "# plt.savefig('All Jaccard Analysis with Different Correlations.tiff', dpi=300, bbox_inches='tight')\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Statistical Analysis via Box Plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_mvee_anis = percentage_change(anisotropy_array[:,0], data_type='numpy')\n",
    "change_global_anis = percentage_change(anisotropy_array[:,1], data_type='numpy')\n",
    "change_arith_anis = percentage_change(anisotropy_array[:,2], data_type='numpy')\n",
    "change_har_anis = percentage_change(anisotropy_array[:,3], data_type='numpy')\n",
    "change_geo_anis = percentage_change(anisotropy_array[:,4], data_type='numpy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract local anisotropies\n",
    "max_length = max(len(arr) for arr in local_anisotropies) # also yields the max number of arrays that the original list is seperated into\n",
    "element_arrays = [np.array([arr[i] if i < len(arr) else np.nan for arr in local_anisotropies]) for i in range(max_length)]\n",
    "\n",
    "print(max_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize local anisotropies\n",
    "anisotropy_dict = {\n",
    "              'MVEE ': anisotropy_array[:,0],\n",
    "              'Global ': anisotropy_array[:,1],\n",
    "              'Local 1 ': element_arrays[0],\n",
    "              'Local 2 ': element_arrays[1],\n",
    "              'Local 3 ': element_arrays[2],\n",
    "              'Harmonic ': anisotropy_array[:,3]\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=anisotropy_dict, var_name='Anisotropies', value_name='Anisotropy', save_title='Anisotropy measures for all AE realizations', xlabel_rot=45)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_anisotropy_dict = {\n",
    "              'MVEE ': np.array(change_mvee_anis),\n",
    "              'Global ': np.array(change_global_anis),\n",
    "              'Harmonic ': np.array(change_har_anis)\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=change_anisotropy_dict, var_name='Anisotropies', value_name='Changes in Anisotropy (%)', save_title='Change in Anisotropy measures for all AE realizations')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_instability_dict = {\n",
    "              'Anchors \\nChanges': np.array(change_anchors),\n",
    "              'Truth Cluster \\nChanges': np.array(change_percentages),\n",
    "              'MVEE \\nAnisotropy': np.array(change_mvee_anis),\n",
    "              'Global \\nAnisotropy': np.array(change_global_anis),\n",
    "              'Harmonic \\nAnisotropy': np.array(change_har_anis),\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=change_instability_dict, var_name='Reassignments', value_name='Changes in Instability Measures (%)', save_title='Change in Instability measures for all AE realizations',\n",
    "         box_width=0.6, xlabel_rot=45)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# arr_changes = np.array(change_percentages)\n",
    "# sorted_data = np.sort(arr_changes)\n",
    "#\n",
    "# # Calculate the 5-point statistic\n",
    "# minimum = np.min(sorted_data)\n",
    "# maximum = np.max(sorted_data)\n",
    "# median = np.median(sorted_data)\n",
    "# q1 = np.percentile(sorted_data, 25)\n",
    "# q3 = np.percentile(sorted_data, 75)\n",
    "# variance = np.var(sorted_data)\n",
    "#\n",
    "# # Display the five-number summary\n",
    "# print(\"Minimum:\", round(minimum, 5))\n",
    "# print(\"First Quartile (Q1):\", round(q1, 5))\n",
    "# print(\"Median (Q2):\", round(median, 5))\n",
    "# print(\"Third Quartile (Q3):\", round(q3, 5))\n",
    "# print(\"Maximum:\", round(maximum, 5))\n",
    "# print(\"Variance:\", round(variance, 5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Latent Space Anisotropy Visualizations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='global', plotter=True)\n",
    "print(global_anis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Limitations of global anisotropy:\n",
    "\n",
    "Clustering: In datasets with multiple clusters, anisotropy might not accurately reflect the separation or relative positioning of these clusters. It may only indicate the overall spread without detailing the intra-cluster and inter-cluster distances.\n",
    "Density Variations: Anisotropy does not capture variations in density within the dataset. Areas of high data point concentration versus sparse regions are not differentiated by this metric.\n",
    "\n",
    "Fixed the above limitations by the introduction of geometric and/or harmonic anisotropy from  local anisotropy calculations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='local', plotter=True)\n",
    "print(local_anis, arithmetic_anis, harmonic_anis, geometric_anis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
