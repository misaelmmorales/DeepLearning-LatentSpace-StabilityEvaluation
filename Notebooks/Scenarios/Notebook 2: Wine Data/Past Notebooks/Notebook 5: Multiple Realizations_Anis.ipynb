{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "import matplotlib as mpl\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from scipy.stats import spearmanr, pearsonr, entropy\n",
    "import libpysal\n",
    "from esda.moran import Moran\n",
    "from scipy.spatial import ConvexHull\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "import scipy.stats\n",
    "from fastcluster import linkage\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from matplotlib.patches import Ellipse\n",
    "import scipy.linalg as linalg\n",
    "import numpy.linalg as la\n",
    "import seaborn as sns\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Assess how \"sensitive\" your autoencoder is to its initialization and get a sense of how stable the learned embeddings are across different training runs. This approach is especially valuable in research or in situations where reproducibility and stability of the model are crucial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def box_plot(dictionary, var_name, value_name, save_title, box_width=0.8, xlabel_rot=0):\n",
    "\n",
    "    # Make dataframe from dictionary\n",
    "    df = pd.DataFrame(dictionary)\n",
    "\n",
    "    # Melt the DataFrame to reshape it\n",
    "    df = pd.melt(df, var_name=var_name, value_name=value_name)\n",
    "\n",
    "    mpl.rcParams['figure.dpi'] = 500\n",
    "    plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "    plt.rcParams['mathtext.fontset'] = 'stix'\n",
    "    plt.figure(figsize=(4,3))\n",
    "    sns.boxplot(data = df, x=var_name, y=value_name, boxprops=dict(alpha=.9),palette=\"muted\",linewidth=0.7,fliersize=0.9, width=box_width)\n",
    "    plt.ylabel(value_name,fontsize=10)\n",
    "    plt.xlabel('')\n",
    "    plt.xticks(rotation=xlabel_rot)\n",
    "    plt.savefig(save_title+'.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show(block=False)\n",
    "    return"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def percentage_change(measure, data_type):\n",
    "\n",
    "    \"\"\"\n",
    "    Finds the percentage change in any measure of choice for each consecutive pair of AE realizations\n",
    "    :param realizations:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    changes = []\n",
    "\n",
    "    if data_type.lower() == 'list':\n",
    "        for i in range(0, len(measure)):\n",
    "            prev_set = set(measure[i - 1])\n",
    "            current_set = set(measure[i])\n",
    "            changed_elements = current_set.symmetric_difference(prev_set)\n",
    "            total_elements = len(prev_set.union(current_set))\n",
    "            change = len(changed_elements) / total_elements * 100\n",
    "            changes.append(change)\n",
    "\n",
    "    elif data_type.lower() == 'numpy':\n",
    "        for i in range(0, len(measure)):\n",
    "            prev_row = measure[i - 1]\n",
    "            current_row = measure[i]\n",
    "            change = np.abs((current_row - prev_row) / prev_row) * 100\n",
    "            changes.append(change)\n",
    "\n",
    "    return changes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that this function is a sequential comparison and not comprehensive across all AE realization pairs. If you want a global stability/instability view, then consider the Jaccard similarity instead or MC approach.\n",
    "\n",
    "The sequential change in percentages is useful for understanding the immediate stability of the model from one realization to the next. It can highlight how sensitive the model is to small changes in the random seed or initialization parameters. On the other hand, the Jaccard similarity across all pairs of realizations offers a broader view of overall stability and consistency across the entire range of realizations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def check_symmetric(a, rtol=1e-05, atol=1e-08):\n",
    "    \"\"\"\n",
    "    This function checks if the distance matrix is symmetric, prior to making a sorted dissimilarity matrix\n",
    "    \"\"\"\n",
    "    return np.allclose(a, a.T, rtol=rtol, atol=atol)\n",
    "\n",
    "\n",
    "def seriation(Z, N, cur_index):\n",
    "    \"\"\"\n",
    "    This is a function that creates a sorted 2D matrix as a figure\n",
    "\n",
    "        input:\n",
    "            - Z is a hierarchical tree (dendrogram)\n",
    "            - N is the number of points given to the clustering process\n",
    "            - cur_index is the position in the tree for the recursive traversal\n",
    "        output:\n",
    "            - order implied by the hierarchical tree Z\n",
    "\n",
    "        seriation computes the order implied by a hierarchical tree (dendrogram)\n",
    "    \"\"\"\n",
    "\n",
    "    if cur_index < N:\n",
    "        return [cur_index]\n",
    "    else:\n",
    "        left = int(Z[cur_index - N, 0])\n",
    "        right = int(Z[cur_index - N, 1])\n",
    "        return seriation(Z, N, left) + seriation(Z, N, right)\n",
    "\n",
    "\n",
    "def compute_serial_matrix(dist_mat, method=\"ward\"):\n",
    "    \"\"\"\n",
    "        input:\n",
    "            - dist_mat is a distance matrix\n",
    "            -  = [\"ward\",\"single\",\"average\",\"complete\"]\n",
    "        output:\n",
    "            - seriated_dist is the input dist_mat,\n",
    "              but with re-ordered rows and columns\n",
    "              according to the seriation, i.e. the\n",
    "              order implied by the hierarchical tree\n",
    "            - res_order is the order implied by\n",
    "              the hierarchical tree\n",
    "            - res_linkage is the hierarchical tree (dendrogram)\n",
    "\n",
    "        compute_serial_matrix transforms a distance matrix into\n",
    "        a sorted distance matrix according to the order implied\n",
    "        by the hierarchical tree (dendrogram)\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(dist_mat)\n",
    "    flat_dist_mat = dist_mat if len(dist_mat.shape) == 2 else squareform(dist_mat)\n",
    "    res_linkage = linkage(flat_dist_mat, method=method, preserve_input=True)\n",
    "    res_order = seriation(res_linkage, N, N + N - 2)\n",
    "    seriated_dist = np.zeros((N, N))\n",
    "    a, b = np.triu_indices(N, k=1)\n",
    "    seriated_dist[a, b] = dist_mat[[res_order[i] for i in a], [res_order[j] for j in b]]\n",
    "    seriated_dist[b, a] = seriated_dist[a, b]\n",
    "\n",
    "    return seriated_dist, res_order, res_linkage"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mvee(points, tol=0.0001):\n",
    "    \"\"\"\n",
    "    Finds the ellipse equation in \"center form\"\n",
    "    (x-c).T * A * (x-c) = 1\n",
    "    \"\"\"\n",
    "    N, d = points.shape\n",
    "    Q = np.column_stack((points, np.ones(N))).T\n",
    "    err = tol+1.0\n",
    "    u = np.ones(N)/N\n",
    "    while err > tol:\n",
    "        # assert u.sum() == 1 # invariant\n",
    "        X = np.dot(np.dot(Q, np.diag(u)), Q.T)\n",
    "        M = np.diag(np.dot(np.dot(Q.T, la.inv(X)), Q))\n",
    "        jdx = np.argmax(M)\n",
    "        step_size = (M[jdx]-d-1.0)/((d+1)*(M[jdx]-1.0))\n",
    "        new_u = (1-step_size)*u\n",
    "        new_u[jdx] += step_size\n",
    "        err = la.norm(new_u-u)\n",
    "        u = new_u\n",
    "    c = np.dot(u, points)\n",
    "    A = la.inv(np.dot(np.dot(points.T, np.diag(u)), points)\n",
    "               - np.multiply.outer(c, c))/d\n",
    "    return A, c\n",
    "\n",
    "\n",
    "def dist_2_cent(x, y, center):\n",
    "    '''\n",
    "    Obtain distance to center coordinates for the entire x,y array passed.\n",
    "    '''\n",
    "\n",
    "    # delta_x, delta_y = abs(x - center[0]), abs(y - center[1])\n",
    "    delta_x, delta_y = (x - center[0]), (y - center[1])\n",
    "    dist = np.sqrt(delta_x ** 2 + delta_y ** 2)\n",
    "\n",
    "    return delta_x, delta_y, dist\n",
    "\n",
    "\n",
    "def get_outer_shell(center, x, y):\n",
    "    '''\n",
    "    Selects those stars located in an 'outer shell' of the points cloud,\n",
    "    according to a given accuracy (ie: the 'delta_angle' of the slices the\n",
    "    circle is divided in).\n",
    "    '''\n",
    "\n",
    "    delta_x, delta_y, dist = dist_2_cent(x, y, center)\n",
    "\n",
    "    # Obtain correct angle with positive x axis for each point.\n",
    "    angles = []\n",
    "    for dx, dy in zip(*[delta_x, delta_y]):\n",
    "        ang = np.rad2deg(np.arctan(abs(dx / dy)))\n",
    "        if dx > 0. and dy > 0.:\n",
    "            angles.append(ang)\n",
    "        elif dx < 0. and dy > 0.:\n",
    "            angles.append(180. - ang)\n",
    "        elif dx < 0. and dy < 0.:\n",
    "            angles.append(270. - ang)\n",
    "        elif dx > 0. and dy < 0.:\n",
    "            angles.append(360. - ang)\n",
    "\n",
    "    # Get indexes of angles from min to max value.\n",
    "    min_max_ind = np.argsort(angles)\n",
    "\n",
    "    # Determine sliced circumference. 'delta_angle' sets the number of slices.\n",
    "    delta_angle = 1.\n",
    "    circle_slices = np.arange(delta_angle, 361., delta_angle)\n",
    "\n",
    "    # Fill outer shell with as many empty lists as slices.\n",
    "    outer_shell = [[] for _ in range(len(circle_slices))]\n",
    "    # Initialize first angle value (0\\degrees) and index of stars in list\n",
    "    # ordered from min to max distance value to center.\n",
    "    ang_slice_prev, j = 0., 0\n",
    "    # For each slice.\n",
    "    for k, ang_slice in enumerate(circle_slices):\n",
    "        # Initialize previous maximum distance and counter of stars that have\n",
    "        # been processed 'p'.\n",
    "        dist_old, p = 0., 0\n",
    "        # For each star in the list, except those already processed (ie: with\n",
    "        # an angle smaller than 'ang_slice_prev')\n",
    "        for i in min_max_ind[j:]:\n",
    "            # If the angle is within the slice.\n",
    "            if ang_slice_prev <= angles[i] < ang_slice:\n",
    "                # Increase the index that stores the number of stars processed.\n",
    "                p += 1\n",
    "                # If the distance to the center is greater than the previous\n",
    "                # one found (if any).\n",
    "                if dist[i] > dist_old:\n",
    "                    # Store coordinates of new star farthest away from center\n",
    "                    # in this slice.\n",
    "                    outer_shell[k] = [x[i], y[i]]\n",
    "                    # Re-assign previous max distance value.\n",
    "                    dist_old = dist[i]\n",
    "            # If the angle value is greater than the max slice value.\n",
    "            elif angles[i] >= ang_slice:\n",
    "                # Increase index of last star processed and break out of\n",
    "                # stars loop.\n",
    "                j += p\n",
    "                break\n",
    "\n",
    "        # Re-assign minimum slice angle value.\n",
    "        ang_slice_prev = ang_slice\n",
    "\n",
    "    # Remove empty lists from array (ie: slices with no stars in it).\n",
    "    outer_shell = np.asarray([x for x in outer_shell if x != []])\n",
    "\n",
    "    return outer_shell\n",
    "\n",
    "\n",
    "def random_points():\n",
    "    mu, sigma = np.random.uniform(-10, 10), np.random.uniform(0., 10)\n",
    "    return mu, sigma\n",
    "\n",
    "\n",
    "def run_mvee(array_2d, plotter=True):\n",
    "\n",
    "    # # Generate some random points to test\n",
    "    # N = 2000\n",
    "    # mux, sigmax = random_points()\n",
    "    # muy, sigmay = random_points()\n",
    "    # x = np.random.normal(mux, sigmax, N)\n",
    "    # y = np.random.normal(muy, sigmay, N)\n",
    "    # center = [mux, muy]\n",
    "    # points = get_outer_shell(center, x, y)\n",
    "\n",
    "    # center the 2d array at 0.0 i.e., mean centering\n",
    "    data_array = array_2d - np.mean(array_2d, axis=0)\n",
    "\n",
    "    # Extract x and y coordinates from mean centered array\n",
    "    x, y = data_array[:, 0], data_array[:, 1]\n",
    "\n",
    "    # Use the centroid of data as the initial center for get_outer_shell\n",
    "    center = [np.mean(x), np.mean(y)]\n",
    "    points = get_outer_shell(center, x, y)\n",
    "\n",
    "    # Run the MVEE algorithm\n",
    "    A, centroid = mvee(points)\n",
    "\n",
    "    # Extract ellipse parameters\n",
    "    U, D, V = la.svd(A)\n",
    "    rx, ry = 1./np.sqrt(D)\n",
    "    dx, dy = 2 * rx, 2 * ry\n",
    "    a, b = max(dx, dy), min(dx, dy)\n",
    "    mvee_anis = a/b\n",
    "    alpha = np.rad2deg(np.arccos(V[0][1]))\n",
    "\n",
    "    if plotter:\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(x, y, s=10, zorder=4)\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=75, c='r', zorder=3)\n",
    "        ax.scatter(*centroid, s=70, c='g')\n",
    "        ax.scatter(center[0], center[1], c='k', s=70)\n",
    "        ellipse = Ellipse(xy=centroid, width=a, height=b, edgecolor='k',\n",
    "                          angle=alpha, fc='None', lw=2)\n",
    "        ax.add_patch(ellipse)\n",
    "        plt.show(block=False)\n",
    "    return mvee_anis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_anisotropy(array_2d, type, plotter=True):\n",
    "\n",
    "    # Initialize variables\n",
    "    global_anis = None\n",
    "    local_anis = []\n",
    "    arithmetic_anis = None\n",
    "    harmonic_anis = None\n",
    "    geometric_anis = None\n",
    "\n",
    "    # Perform mean centering to 0 on array\n",
    "    temp = array_2d.copy()\n",
    "    data = temp - np.mean(temp, axis=0)\n",
    "    center = np.mean(data, axis=0)\n",
    "\n",
    "    # Estimate the PDF using Gaussian Kernel Density Estimation\n",
    "    kde = gaussian_kde(data.T)\n",
    "    x_grid, y_grid = np.mgrid[data[:,0].min():data[:,0].max():100j, data[:,1].min():data[:,1].max():100j]\n",
    "    pdf_values = kde(np.vstack([x_grid.ravel(), y_grid.ravel()]))\n",
    "\n",
    "    # Reshape for contour plot\n",
    "    pdf_values = pdf_values.reshape(x_grid.shape)\n",
    "\n",
    "    # Make the plot\n",
    "    if plotter:\n",
    "        # # Plotting the PDF and data\n",
    "        # fig, ax = plt.subplots()\n",
    "        # ax.imshow(np.rot90(pdf_values), cmap=plt.cm.gist_earth_r, extent=[x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()])\n",
    "        # ax.scatter(center[0], center[1], c='r', s=2)\n",
    "        # ax.plot(data[:, 0], data[:, 1], 'k.', markersize=2)\n",
    "        # Create a grid layout for the plots\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        gs = gridspec.GridSpec(4, 4)\n",
    "\n",
    "        # Main scatter plot\n",
    "        ax_main = plt.subplot(gs[1:4, 0:3])\n",
    "        ax_main.imshow(np.rot90(pdf_values), cmap=plt.cm.gist_earth_r, extent=[x_grid.min(), x_grid.max(), y_grid.min(), y_grid.max()])\n",
    "        ax_main.scatter(center[0], center[1], c='r', s=2)\n",
    "        ax_main.plot(data[:, 0], data[:, 1], 'k.', markersize=2)\n",
    "        ax_main.set_xlabel('LS 1')\n",
    "        ax_main.set_ylabel('LS 2')\n",
    "\n",
    "        # Marginal distributions\n",
    "        ax_xDist = plt.subplot(gs[0, 0:3], sharex=ax_main)\n",
    "        ax_yDist = plt.subplot(gs[1:4, 3], sharey=ax_main)\n",
    "\n",
    "        # # Plotting marginal distributions as histograms\n",
    "        # ax_xDist.hist(data[:, 0], bins=30, orientation='vertical', color='blue')\n",
    "        # ax_xDist.set_ylabel('Frequency (LS 1)')\n",
    "        #\n",
    "        # ax_yDist.hist(data[:, 1], bins=30, orientation='horizontal', color='blue')\n",
    "        # ax_yDist.set_xlabel('Frequency (LS 2)')\n",
    "\n",
    "        # Plotting marginal distributions as KDE's\n",
    "        kde_x = gaussian_kde(data[:, 0])\n",
    "        x_line = np.linspace(data[:, 0].min(), data[:, 0].max(), 100)\n",
    "        ax_xDist.plot(x_line, kde_x(x_line), color='blue')\n",
    "        ax_xDist.set_ylabel('Density (LS 1)')\n",
    "\n",
    "        kde_y = gaussian_kde(data[:, 1])\n",
    "        y_line = np.linspace(data[:, 1].min(), data[:, 1].max(), 100)\n",
    "        ax_yDist.plot(kde_y(y_line), y_line, color='blue')\n",
    "        ax_yDist.set_xlabel('Density (LS 2)')\n",
    "\n",
    "        # Turn off ticks for marginal distributions\n",
    "        ax_xDist.xaxis.set_tick_params(labelbottom=False)\n",
    "        ax_yDist.yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "\n",
    "    if type.lower() == 'global':\n",
    "        # Compute covariance matrix for the entire dataset\n",
    "        cov = np.cov(data.T)\n",
    "        eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "        order = eigenvalues.argsort()[::-1]\n",
    "        eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "        angle = np.degrees(np.arctan2(*eigenvectors[:, 0][::-1]))\n",
    "\n",
    "        # Ellipse parameters for the entire dataset\n",
    "        width, height = 2 * np.sqrt(eigenvalues)\n",
    "        overall_ellipse = Ellipse(xy=np.mean(data, axis=0), width=width, height=height, angle=angle, edgecolor='red', fc='None', lw=2)\n",
    "\n",
    "        # Compute anisotropy global\n",
    "        global_anis = width / height\n",
    "\n",
    "    elif type.lower() == 'local':\n",
    "        # Contour level at 95% confidence interval\n",
    "        level = np.percentile(pdf_values, 95)\n",
    "\n",
    "        if plotter:\n",
    "            # Find and fit ellipses to all such contours, which is indicative of tentative cluster structures within an array if any\n",
    "            for path in ax_main.contour(x_grid, y_grid, pdf_values, levels=[level], colors='r').collections[0].get_paths():\n",
    "                v = path.vertices\n",
    "                cov = np.cov(v, rowvar=False)\n",
    "                eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "                order = eigenvalues.argsort()[::-1]\n",
    "                eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "                angle = np.arctan2(*eigenvectors[:, 0][::-1])\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ellipse = Ellipse(xy=np.mean(v, axis=0), width=width, height=height, angle=np.degrees(angle), edgecolor='b', fc='None')\n",
    "\n",
    "                # Add each ellipse to the plot\n",
    "                if plotter:\n",
    "                    #ax.add_patch(ellipse)\n",
    "                    ax_main.add_patch(ellipse)\n",
    "\n",
    "                # Compute local anisotropies with respect to bi or multimodal pdf's and save\n",
    "                local_anis.append(width / height)\n",
    "        else:\n",
    "            # Find and fit ellipses to all such contours, which is indicative of tentative cluster structures within an array if any\n",
    "            # for path in plt.contour(x_grid, y_grid, pdf_values, levels=[level], colors='r').collections[0].get_paths():\n",
    "            #     v = path.vertices\n",
    "            contours = measure.find_contours(pdf_values, level=level)\n",
    "            for contour in contours:\n",
    "                v = contour\n",
    "                cov = np.cov(v, rowvar=False)\n",
    "                eigenvalues, eigenvectors = linalg.eigh(cov)\n",
    "                order = eigenvalues.argsort()[::-1]\n",
    "                eigenvalues, eigenvectors = eigenvalues[order], eigenvectors[:, order]\n",
    "                angle = np.arctan2(*eigenvectors[:, 0][::-1])\n",
    "                width, height = 2 * np.sqrt(eigenvalues)\n",
    "                ellipse = Ellipse(xy=np.mean(v, axis=0), width=width, height=height, angle=np.degrees(angle), edgecolor='b', fc='None')\n",
    "\n",
    "                # Compute local anisotropies with respect to bi or multimodal pdf's and save\n",
    "                local_anis.append(width / height)\n",
    "\n",
    "    # Compute the harmonic and geometric means of the local anisotropy\n",
    "    arithmetic_anis = np.mean(local_anis) if local_anis else None\n",
    "    harmonic_anis = scipy.stats.hmean(local_anis) if local_anis else None\n",
    "    geometric_anis = scipy.stats.gmean(local_anis) if local_anis else None\n",
    "\n",
    "    if  plotter and type.lower() == 'global':\n",
    "        #ax.add_patch(overall_ellipse)\n",
    "        ax_main.add_patch(overall_ellipse)\n",
    "\n",
    "    if plotter:\n",
    "        plt.show(block=False)\n",
    "\n",
    "\n",
    "    return global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The modified_stress function is more about comparing the embeddings point-by-point, while the adjusted_stress function is about comparing the overall structure of the embeddings."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def modified_stress(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Compute raw stress between two sets of embeddings using Euclidean distance.\n",
    "\n",
    "    Args:\n",
    "        embeddings1 (numpy.ndarray): Array of embeddings from the first autoencoder.\n",
    "        embeddings2 (numpy.ndarray): Array of embeddings from the second autoencoder.\n",
    "\n",
    "    Returns:\n",
    "        float: Stress value indicating dissimilarity between the embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings have the same shape\n",
    "    if embeddings1.shape != embeddings2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape\")\n",
    "\n",
    "    squared_diff = np.sum((embeddings1 - embeddings2)**2, axis=1)\n",
    "\n",
    "    stress = np.mean(np.sqrt(squared_diff))\n",
    "    return stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def adjusted_stress(embeddings1, embeddings2):\n",
    "    \"\"\"\n",
    "    Compute normalized adjusted stress between two sets of embeddings.\n",
    "\n",
    "    Args:\n",
    "        embeddings1 (numpy.ndarray): Array of embeddings from the first autoencoder.\n",
    "        embeddings2 (numpy.ndarray): Array of embeddings from the second autoencoder.\n",
    "\n",
    "    Returns:\n",
    "        float: Adjusted MDS stress indicating dissimilarity between the embeddings.\n",
    "    \"\"\"\n",
    "    # Ensure the embeddings have the same shape\n",
    "    if embeddings1.shape != embeddings2.shape:\n",
    "        raise ValueError(\"Embeddings must have the same shape\")\n",
    "\n",
    "    dists1 = squareform(pdist(embeddings1, 'euclidean'))\n",
    "    dists2 = squareform(pdist(embeddings2, 'euclidean'))\n",
    "\n",
    "    numerator = np.sum((dists1 - dists2) ** 2)\n",
    "    denominator = np.sum(dists1 ** 2)\n",
    "\n",
    "    if denominator == 0:\n",
    "        return 0.0\n",
    "\n",
    "    stress = np.sqrt(numerator / denominator)\n",
    "    return stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "target = load_wine().target\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], idx"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 2000\n",
    "batch_size = 16\n",
    "#dataset = torch.tensor(df_scaled, dtype=torch.float32)\n",
    "dataset = CustomDataset(torch.tensor(df_scaled, dtype=torch.float32))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, encoding_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_dim = df.shape[1]\n",
    "encoding_dim = 2\n",
    "model = Autoencoder(input_dim, encoding_dim)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "initial_seed = 0 # starting sequence for random seed generator\n",
    "step_size = 10   # Change this to increase seeds every x units as needed\n",
    "random.seed(initial_seed)\n",
    "num_seeds = 100 # 6 # 1000 # nos of realizations needed\n",
    "seed_values = [initial_seed + (step_size * i) for i in range(num_seeds)]\n",
    "cluster_changes = np.zeros_like(target)  # To track changes in cluster assignments\n",
    "change_percentages = []  # List to store percentage of changed clusters for each realization\n",
    "statistics_array = np.empty((num_seeds, 7))\n",
    "pvalues_array = np.empty((num_seeds, 3))\n",
    "convexhull_array = np.empty((num_seeds, 3))\n",
    "anisotropy_array = np.empty((num_seeds, 5))\n",
    "convex_hull_vertices = []\n",
    "all_loss_curves = []\n",
    "local_anisotropies = []\n",
    "hdf5_file = 'embedding_realizations.h5' # store each latent space for all AE realizations here\n",
    "\n",
    "# Make the HDF5 file\n",
    "with h5py.File(hdf5_file, 'w') as f:\n",
    "    pass  # Just to create the file\n",
    "\n",
    "# Make 3x3 subplot for 6 randomly selected AE realizations visualizations\n",
    "fig, axs = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axs = axs.ravel()\n",
    "random.seed(initial_seed)\n",
    "visualization_indices = random.sample(range(num_seeds), 6)\n",
    "\n",
    "for idx, seed in enumerate(seed_values):\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    generator = torch.Generator().manual_seed(seed)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, generator=generator)\n",
    "\n",
    "    model = Autoencoder(input_dim, encoding_dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    epoch_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        #for data in dataloader:\n",
    "        for data, indices in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epoch_losses.append(total_loss / len(dataloader))\n",
    "    all_loss_curves.append(epoch_losses)\n",
    "\n",
    "################################################ SELF NOTE #############################################################################################################\n",
    "#     # Clustering in latent space\n",
    "#     Here's a breakdown of how it works:\n",
    "#\n",
    "# 1. **Initialization**:\n",
    "#    - `cluster_changes` is initialized as a zero array with the same shape as the `target` (truth labels). This means initially, all data points have 0 changes.\n",
    "#    - `predicted_clusters` is computed for each realization after clustering in the latent space.\n",
    "#\n",
    "# 2. **Matching clusters with truth labels**:\n",
    "#    - A cost matrix is computed to find the best correspondence between predicted clusters and the true labels.\n",
    "#    - The Hungarian algorithm (`linear_sum_assignment`) is used to determine the optimal assignment.\n",
    "#    - After the best assignment is determined, `predicted_clusters` labels are rearranged to match the true labels as closely as possible.\n",
    "#\n",
    "# 3. **Tracking changes**:\n",
    "#    - For each data point, if its assigned cluster (`predicted_clusters`) doesn't match its true label (`target`), its corresponding value in `cluster_changes` is incremented by 1.\n",
    "#    - This process is repeated for all realizations.\n",
    "########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    # with torch.no_grad():\n",
    "    #     encoded_data = model.encoder(dataset).numpy()\n",
    "\n",
    "    # Reorder the embeddings gotten based on the originlal data index\n",
    "    with torch.no_grad():\n",
    "        encoded_data, original_indices = zip(*[(model.encoder(d[None, ...]), idx) for d, idx in dataset])\n",
    "        # Reordering step here\n",
    "        encoded_data = torch.cat(encoded_data, dim=0).numpy()\n",
    "        order = np.argsort(original_indices)\n",
    "        encoded_data = encoded_data[order]\n",
    "\n",
    "        # Save each latent space embeddings to an HDF5 file\n",
    "        with h5py.File(hdf5_file, 'a') as f:\n",
    "            f.create_dataset(f'embedding_realizations_{idx}', data=encoded_data)\n",
    "\n",
    "\n",
    "################################################ SELF NOTE #############################################################################################################\n",
    "# The CustomDataset class returns both the data and its index. When shuffled in the DataLoader, the indices will follow the same shuffling order as the data.\n",
    "# During training, you get both data and indices from the DataLoader. However, the indices are not used in training; they're only needed for evaluation.\n",
    "\n",
    "    # When evaluating, you need to reorder the embeddings according to the original indices. This is done by first getting the embeddings and their original indices, and then using np.argsort to reorder the embeddings to match the original data order.\n",
    "\n",
    "    # This approach ensures that, despite shuffling during training, you can align the embeddings from different autoencoders to their corresponding original data points. This alignment is crucial for comparing embeddings across different training runs with different seeds.\n",
    "########################################################################################################################################################################\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=3, n_init=10).fit(encoded_data)\n",
    "    predicted_clusters = kmeans.labels_\n",
    "\n",
    "    # Matching predicted clusters to true clusters\n",
    "    cost_matrix = -np.array([[np.sum((predicted_clusters[true_indices] == pred_cluster))\n",
    "                              for pred_cluster in range(3)]\n",
    "                             for true_indices in [np.where(target == i)[0] for i in range(3)]])\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "\n",
    "    for i in range(3):\n",
    "        predicted_clusters[predicted_clusters == col_ind[i]] = -(i+10)  # Temporarily set to negative values\n",
    "    for i in range(3):\n",
    "        predicted_clusters[predicted_clusters == -(i+10)] = i\n",
    "\n",
    "    changes = (predicted_clusters != target)\n",
    "    change_percentages.append(100 * np.sum(changes) / len(target))\n",
    "    cluster_changes += changes\n",
    "\n",
    "    # Compute the chaos, global, and spatial statistics for each of the latent spaces for each AE realizations\n",
    "    # Chaos Statistics\n",
    "    array_flat = encoded_data.flatten()\n",
    "    values, counts = np.unique(array_flat, return_counts=True)\n",
    "    probabilities = counts / counts.sum()\n",
    "    entropy_values = entropy(probabilities, base=2) # s1\n",
    "\n",
    "    # Global Statistics\n",
    "    mean_global = np.mean(encoded_data) # s2\n",
    "    variance_global = np.var(encoded_data) # s3\n",
    "    norm_fro = np.linalg.norm(encoded_data, 'fro') # s4\n",
    "    pearson, coef_pval = pearsonr(encoded_data[:,0], encoded_data[:,1]) # s5\n",
    "    spearman, rank_pval = spearmanr(encoded_data[:,0], encoded_data[:,1]) # s6\n",
    "\n",
    "    # Spatial Statistics\n",
    "    weights = libpysal.weights.lat2W(encoded_data.shape[0],encoded_data.shape[1], rook=False, id_type=\"float\")\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    mi = Moran(encoded_data, weights) # two_tailed=False for one tailed Moran, emeasure of overall spatial autocorrelation -1: perfect clustering of dissimilar values/objects (perfect dispersion), 0: perfect randomness, 1: perfect clustering of similar values/objects.\n",
    "    moran = mi.I # s7\n",
    "    moran_pval = mi.p_norm\n",
    "\n",
    "    # Store the computed statistics in the array for this iteration\n",
    "    statistics_array[idx, :] = [entropy_values, mean_global, variance_global, norm_fro, pearson, spearman, moran]\n",
    "    pvalues_array[idx, :] = [coef_pval, rank_pval, moran_pval]\n",
    "\n",
    "    # Save statistics and pvalues array to numpy files\n",
    "    np.save('statistics_array.npy', statistics_array)\n",
    "    np.save('pvalues_array.npy', pvalues_array)\n",
    "\n",
    "    # Compute convex hull and associated attributes\n",
    "    my_points = encoded_data.copy()\n",
    "    hull = ConvexHull(my_points)\n",
    "    vertices_idx = hull.vertices\n",
    "    vertices = my_points[hull.vertices]\n",
    "    polygon = Polygon(vertices)\n",
    "\n",
    "    # Store computed variables needed from convex hull polygon i.e., the number of vertices, area, and volume for each AE realization\n",
    "    convexhull_array[idx, :] = [len(vertices), hull.area, hull.volume]\n",
    "    convex_hull_vertices.append(vertices_idx)\n",
    "\n",
    "    # Save convex hull attributes to numpy files\n",
    "    np.save('convexhull_array.npy', convexhull_array)\n",
    "\n",
    "    # Different types of anisotropy computations\n",
    "    mvee_anis= run_mvee(array_2d=encoded_data, plotter=False)\n",
    "    global_anis, _, _, _, _ = compute_anisotropy(array_2d=encoded_data, type='global', plotter=False)\n",
    "    _, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='local', plotter=False)\n",
    "    local_anisotropies.append(np.array(local_anis))\n",
    "    anisotropy_array[idx, :] = [mvee_anis, global_anis, arithmetic_anis, harmonic_anis, geometric_anis]\n",
    "\n",
    "    # Save and store anisotropy attributes to numpy files or whatever format as needed\n",
    "    np.save('anisotropy_array.npy', anisotropy_array)\n",
    "\n",
    "    # Check if the current realization index is in the list of visualization_indices\n",
    "    if idx in visualization_indices:\n",
    "        # Visualization of latent space in 3x3 grid\n",
    "        axs[visualization_indices.index(idx)].scatter(encoded_data[:, 0], encoded_data[:, 1], c=predicted_clusters, cmap='viridis', marker='o', s=30,\n",
    "                      alpha=0.7)\n",
    "        axs[visualization_indices.index(idx)].set_title(f\"Realization {idx + 1}, Seed: ${seed}$\")\n",
    "\n",
    "        for simplex in hull.simplices:\n",
    "            axs[visualization_indices.index(idx)].plot(my_points[simplex, 0], my_points[simplex, 1], 'r--')\n",
    "            #axs[visualization_indices.index(idx)].fill(my_points[hull.vertices, 0], my_points[hull.vertices, 1], c='yellow', alpha=0.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('LS Visualization for Select AE Realizations with CH.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Recall in each realization, we do the following:**\n",
    "\n",
    "1. Train an autoencoder, and then applying k-means to the latent space to identify clusters.\n",
    "2. These clusters are then matched to the true labels to minimize the discrepancies between predicted and true cluster assignments.\n",
    "3. After matching, for each data point, if the matched predicted cluster is different from its true label, it's considered a \"change\", and the corresponding entry in `cluster_changes` is incremented.\n",
    "\n",
    "The `cluster_changes` array accumulates these \"changes\" for each data point over all realizations. After all realizations are done, the value for a data point in `cluster_changes` represents how many times (out of all realizations) the predicted cluster for that data point didn't match its true label. This becomes a tell-tale sign of stability in the latent feature space.\n",
    "\n",
    "The introduction of stochastic uncertainty through changing the random seed is a way to account for and analyze the variability in model training and performance. By training a model multiple times with different seeds, you can assess the robustness of the model and gain insights into how sensitive it is to variations in initialization and training data presentation. It helps in understanding the model's generalization and can be useful for hyperparameter tuning and model evaluation.\n",
    "\n",
    "\n",
    "**Advantages of Minibatch Training for Small Datasets:**\n",
    "\n",
    "1. Regularization: Minibatch training introduces noise into the training process, which can act as a form of regularization. This can help prevent overfitting, even on small datasets.\n",
    "2. Efficiency: Minibatch training allows you to take advantage of parallelism, such as using GPUs, which can significantly speed up training even for small datasets.\n",
    "3. Convergence: Minibatch training can lead to faster convergence, which means your model reaches a good solution more quickly.\n",
    "4. Memory Efficiency: Training on the full dataset might not fit into memory, especially when the dataset is very large. Minibatch training allows you to work with smaller portions of the data at a time.\n",
    "\n",
    "**Considerations for Minibatch Training on Small Datasets:**\n",
    "\n",
    "1. Batch Size: When working with a small dataset, you should carefully choose the batch size. Very small batch sizes can introduce high variance in gradient updates, while very large batch sizes may not provide the regularization benefits of minibatch training. You may need to experiment to find the optimal batch size for your specific dataset and model.\n",
    "2. Learning Rate: Smaller batch sizes may require smaller learning rates to prevent overshooting during optimization. Again, hyperparameter tuning may be necessary.\n",
    "3. Validation Set: With a small dataset, it's crucial to have a validation set to monitor model performance. Overfitting can still occur, especially if your model has many parameters.\n",
    "4. Data Augmentation: Data augmentation techniques (e.g., flipping, rotating, cropping) can help artificially increase the effective size of your dataset, making minibatch training more effective.\n",
    "5. Transfer Learning: If your small dataset is similar to a larger dataset, you can consider using pre-trained models and fine-tuning them on your dataset.\n",
    "6. In summary, while minibatch training can be beneficial for small datasets, it requires careful consideration of batch size, learning rate, and other hyperparameters. Additionally, techniques like data augmentation and transfer learning can be used to make the most of small datasets. Ultimately, the choice of whether to use minibatch training or not depends on the specific problem and constraints you are dealing with.\n",
    "\n",
    "\n",
    "Idea of using the convex hull vertices to study the variability of inferences in different autoencoder realizations is quite innovative and can be a valuable approach. Here's why this could be beneficial and how you might go about it:\n",
    "\n",
    "\n",
    "**Advantages of Using Convex Hull Vertices:**\n",
    "1. Stability Analysis: By examining how the vertices of the convex hull change with different random seeds, you can assess the stability and robustness of the autoencoder. Consistent vertices across different realizations suggest that the autoencoder is reliably capturing the same underlying structure in the data.\n",
    "2. Identifying Anchor Points: Convex hull vertices can serve as \"anchor points\" in your latent space, providing a way to understand the extremities or boundaries of the representations learned by the autoencoder.\n",
    "3. Quantitative Analysis: Changes in the vertices can be quantified, allowing for a more systematic analysis of variability. Metrics like the distance between corresponding vertices in different realizations, or the area/volume of the convex hulls, can provide insights into the variability induced by different seeds.\n",
    "\n",
    "\n",
    "**Implementing the Analysis:**\n",
    "1. Compute Convex Hull: For each autoencoder realization, use the encoded latent space representations to compute the convex hull. Tools like scipy.spatial.ConvexHull in Python can be useful for this.\n",
    "2. Compare Vertices: Analyze how the vertices of these convex hulls vary across different realizations. You might consider:\n",
    "    * The position of vertices.\n",
    "    * The number of vertices.\n",
    "    * Geometric properties like the area or volume enclosed by the convex hull.\n",
    "3. Statistical Analysis: Perform statistical tests or visualizations to understand the degree of variability. For instance, plotting the convex hulls from different realizations can visually show the differences.\n",
    "4. Correlation with Seeds: Investigate if there's any pattern or correlation between the changes in the convex hull and the random seeds used. This might reveal how sensitive the autoencoder's latent space is to initialization.\n",
    "\n",
    "**Considerations:**\n",
    "1. Dimensionality: If your latent space is high-dimensional, the interpretation of convex hulls can become complex. Consider dimensionality reduction techniques for visualization and analysis.\n",
    "2. Comparability: Ensure that the method used for comparing vertices is consistent and meaningful across different realizations.\n",
    "\n",
    "\n",
    "In summary, using the convex hull vertices to analyze the impact of different random seeds on the latent space of an autoencoder is a promising approach. It can provide insights into the stability and variability of the autoencoder's learned representations, aiding in the understanding of how different initializations affect the model's inferences.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualize training loss for select AE realizations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Generate alpha values within the range [0, 1]\n",
    "alphas = [i/num_seeds for i in range(1, num_seeds + 1)]\n",
    "\n",
    "# Visualize loss functions for select realizations\n",
    "for idx, losses in enumerate(all_loss_curves):\n",
    "    if idx in visualization_indices:\n",
    "        plt.plot(losses, label=f'Realization #{idx+1}', alpha=alphas[idx])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Training Loss over Epochs for Select AE Realizations')\n",
    "plt.legend()\n",
    "plt.savefig('Training loss over select realizations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss_val = np.min(all_loss_curves) # smallest loss from all AE relaizations\n",
    "loss_valani"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute the percentage changes in anchors found via convex hull\n",
    "change_anchors = percentage_change(convex_hull_vertices, data_type='list')\n",
    "\n",
    "# Compute the percentage changes in anisotropies found\n",
    "change_mvee_anis = percentage_change(anisotropy_array[:,0], data_type='numpy')\n",
    "change_global_anis = percentage_change(anisotropy_array[:,1], data_type='numpy')\n",
    "change_arith_anis = percentage_change(anisotropy_array[:,2], data_type='numpy')\n",
    "change_har_anis = percentage_change(anisotropy_array[:,3], data_type='numpy')\n",
    "change_geo_anis = percentage_change(anisotropy_array[:,4], data_type='numpy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize percentage changes for cluster reassigned points #-o\n",
    "plt.plot(range(1, num_seeds + 1), change_percentages, '-')\n",
    "plt.xlabel('AE Realizations')\n",
    "plt.ylabel('Changes in Reassigned Points (%)')\n",
    "plt.title('Instability in Cluster Assignments')\n",
    "plt.xticks(np.arange(0, num_seeds + 1, 100))\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize percentage changes for shape-based statistics\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4)) #-o\n",
    "\n",
    "# First subplot for change anchors\n",
    "axs[0].plot(range(1, num_seeds + 1), change_anchors, '-')\n",
    "axs[0].set_xlabel('AE Realizations')\n",
    "axs[0].set_ylabel('Changes in Reassigned Anchors (%)')\n",
    "axs[0].set_title('Instability in Anchor Points')\n",
    "axs[0].set_xticks(np.arange(0, num_seeds + 1, 100))\n",
    "axs[0].set_ylim([0, 100])\n",
    "\n",
    "axs[1].plot(range(1, num_seeds + 1), change_mvee_anis, '-', label='MVEE Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_global_anis, '-', label='Global Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_arith_anis, '-', label='Arithmetic Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_har_anis, '-', label='Harmonic Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_geo_anis, '-', label='Geometric Anisotropy')\n",
    "axs[1].set_xlabel('AE Realizations')\n",
    "axs[1].set_ylabel('Sequential Changes in Anisotropy (%)')\n",
    "axs[1].set_title('Instability in Anisotropy Statistics')\n",
    "axs[1].set_xticks(np.arange(0, num_seeds + 1, 100))\n",
    "# axs[1].set_ylim([0, 100])\n",
    "\n",
    "plt.legend()\n",
    "plt.subplots_adjust(left=0.0, bottom=0.3, right=1.0, top=1.3, wspace=0.25, hspace=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize percentage changes for anchors and data pooint assignment in clusters\n",
    "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "# First subplot for change anchors\n",
    "axs[0].plot(range(1, num_seeds + 1), change_anchors, '-')\n",
    "axs[0].set_xlabel('AE Realizations')\n",
    "axs[0].set_ylabel('Changes in Reassigned Anchors (%)')\n",
    "axs[0].set_title('Instability in Anchor Points')\n",
    "axs[0].set_xticks(np.arange(0, num_seeds + 1, 100))\n",
    "axs[0].set_ylim([0, 100])\n",
    "\n",
    "axs[1].plot(range(1, num_seeds + 1), change_mvee_anis, '-', label='MVEE Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_global_anis, '-', label='Global Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_arith_anis, '-', label='Arithmetic Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_har_anis, '-', label='Harmonic Anisotropy')\n",
    "axs[1].plot(range(1, num_seeds + 1), change_geo_anis, '-', label='Geometric Anisotropy')\n",
    "axs[1].set_xlabel('AE Realizations')\n",
    "axs[1].set_ylabel('Sequential Changes in Anisotropy (%)')\n",
    "axs[1].set_title('Anisptropy Statistics')\n",
    "axs[1].set_xticks(np.arange(0, num_seeds + 1, 100))\n",
    "axs[1].set_ylim([0, 100])\n",
    "\n",
    "# Second subplot for point reassignment in clusters\n",
    "axs[2].plot(range(1, num_seeds + 1), change_percentages, '-')\n",
    "axs[2].set_xlabel('AE Realizations')\n",
    "axs[2].set_ylabel('Changes in Reassigned Points (%)')\n",
    "axs[2].set_title('Instability in Cluster Assignments')\n",
    "axs[2].set_xticks(np.arange(0, num_seeds + 1, 100))\n",
    "axs[2].set_ylim([0, 100])\n",
    "\n",
    "axs[1].legend()\n",
    "plt.subplots_adjust(left=0.0, bottom=0.3, right=1.3, top=1.3, wspace=0.3, hspace=0.3)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stress Computations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Stress computation for unique pairs only\n",
    "with h5py.File(hdf5_file, 'r') as f:\n",
    "    N = len(f.keys())\n",
    "    # Initialize matrices\n",
    "    adjusted_stress_matrix = np.zeros((N, N))\n",
    "    modified_stress_matrix = np.zeros((N, N))\n",
    "\n",
    "    for i in range(N):\n",
    "        embedding_i = f[f'embedding_realizations_{i}'][:]\n",
    "        # scaler = StandardScaler()\n",
    "        # embedding_i = scaler.fit_transform(embedding_i)\n",
    "        for j in range(i + 1, N):  # Starting from i + 1 to avoid redundant computations\n",
    "            embedding_j = f[f'embedding_realizations_{j}'][:]\n",
    "            # scaler = StandardScaler()\n",
    "            # embedding_j = scaler.fit_transform(embedding_j)\n",
    "            # Compute adjusted stress\n",
    "            adj_stress = adjusted_stress(embedding_i, embedding_j)\n",
    "            adjusted_stress_matrix[i, j] = adj_stress\n",
    "            adjusted_stress_matrix[j, i] = adj_stress\n",
    "\n",
    "            # Compute modified stress\n",
    "            mod_stress = modified_stress(embedding_i, embedding_j)\n",
    "            modified_stress_matrix[i, j] = mod_stress\n",
    "            modified_stress_matrix[j, i] = mod_stress"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize stress matrices for autoencoder realizations\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(adjusted_stress_matrix, annot=False, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Adjusted Stress Matrix between Different AE Realizations\")\n",
    "plt.xlabel(\"Realization Index\")\n",
    "plt.ylabel(\"Realization Index\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(modified_stress_matrix, annot=False, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Modified Stress Matrix between Different AE Realizations\")\n",
    "plt.xlabel(\"Realization Index\")\n",
    "plt.ylabel(\"Realization Index\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize sorted Jaccard matrix\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(adjusted_stress_matrix,\"ward\")\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "#subplot 1\n",
    "im1 = ax1.imshow(ordered_dist_mat, cmap='viridis')\n",
    "\n",
    "# Figure info\n",
    "ax1.set_aspect('auto')\n",
    "ax1.set_title('Sorted Jaccard Similarity Matrix for AE Realizations', size=12)\n",
    "ax1.set_xlabel('Sorted realization index, $i$', size=12)\n",
    "ax1.set_ylabel('Sorted realization index, $j$', size=12)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "\n",
    "# Aesthetics\n",
    "axis = fig.add_axes([0.85, 0.30, 0.04, 1.0])  # Left,bottom, width, length\n",
    "cbar = fig.colorbar(im1, cax=axis, orientation='vertical')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.3, right=0.8, top=1.3, wspace=0.25, hspace=0.3)\n",
    "# plt.savefig('Sorted Similarity Matrices.png', dpi=300, bbox_inches='tight')\n",
    "ax1.grid(visible=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Jaccard Similarity Computation on Anchors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The choice of Jaccard similarity for analyzing the vertices of convex hulls across different realizations of an autoencoder is primarily due to its suitability for comparing sets, especially when the elements of these sets are categorical or discrete, like indices in this case. Here are some key reasons why Jaccard similarity is a good fit for this analysis:\n",
    "\n",
    "1. **Measuring Overlap in Sets:** Jaccard similarity is specifically designed to measure the similarity between finite sets. It's calculated as the size of the intersection divided by the size of the union of the sets. In the context of convex hull vertices, it quantifies how many vertices are common between any two realizations relative to the total unique vertices in both.\n",
    "\n",
    "2. **Applicability to Your Analysis:** In your scenario, you're comparing sets of indices (vertices of convex hulls). Jaccard similarity directly addresses this by evaluating how similar these sets are across different realizations, providing a clear and interpretable metric for the stability of your model's latent space representations.\n",
    "\n",
    "3. **Robustness to Size Variations:** The Jaccard index is robust to variations in the size of the sets. It's particularly useful when the number of vertices in the convex hulls might change across realizations, as it normalizes the similarity by the union of the sets.\n",
    "\n",
    "4. **Intuitive and Interpretable:** The Jaccard similarity yields a value between 0 and 1, where 1 indicates identical sets and 0 indicates no common elements. This makes it easy to interpret in terms of similarity or dissimilarity.\n",
    "\n",
    "5. **Widely Used in Comparative Analysis:** Jaccard similarity is a well-established metric in various fields including biology (for species similarity), text analysis (similarity of documents), and machine learning (comparing clusters or sets), making it a reliable choice.\n",
    "\n",
    "Given these properties, Jaccard similarity provides a straightforward and effective way to compare the convex hull vertices across different model realizations and to gauge the stability and consistency of the model's inferences.\n",
    "\n",
    "\n",
    "Jaccard Similarity of 1: A Jaccard similarity score of 1 means that the sets of vertices (convex hulls) being compared are exactly the same. This would mean that two realizations of the autoencoder have produced identical sets of vertices for their convex hulls. This indicates total stability, as the model's latent space representation is consistent across these AE realizations.\n",
    "\n",
    "Jaccard Similarity of 0: Conversely, a Jaccard similarity score of 0 means that there is no overlap between the sets; they are completely different. For my application, this implies total instability, as the sets of vertices (anchor points in the latent space) for the convex hulls in these two realizations do not match at all.\n",
    "Therefore, in terms of evaluating the stability of the model's latent space representations across different AE realizations:\n",
    "\n",
    "High Jaccard scores (close to 1) across most pairs of realizations suggest a high degree of stability. (Hence the use of neffective to determine how many AE realizations is needed to attain stability in the latent space for inference making)\n",
    "\n",
    "Lower Jaccard scores (especially those close to 0) indicate more variability or instability in how the model is representing the data under different initializations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# To compare the sets of vertices from the convex hulls across different realizations, use set operations. The idea is to assess the overlap and differences between the sets of indices for each pair of realizations.\n",
    "\n",
    "comparisons = {}\n",
    "for (i, vertices_i), (j, vertices_j) in combinations(enumerate(convex_hull_vertices), 2):\n",
    "    set_i, set_j = set(vertices_i.tolist()), set(vertices_j.tolist())\n",
    "    intersection = set_i.intersection(set_j)\n",
    "    union = set_i.union(set_j)\n",
    "    jaccard_similarity = len(intersection) / len(union) if union else 1\n",
    "    comparisons[(i, j)] = jaccard_similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Compute Jaccard similarity matrix\n",
    "num_realizations = len(convex_hull_vertices)\n",
    "similarity_matrix = np.zeros((num_realizations, num_realizations))\n",
    "\n",
    "for (i, j), similarity in comparisons.items():\n",
    "    similarity_matrix[i, j] = similarity\n",
    "    similarity_matrix[j, i] = similarity  # Mirror the similarity as the matrix is symmetric"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize the jaccard similarity scores via heat map\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, annot=False, cmap='viridis')\n",
    "plt.title('Jaccard Similarity Between Different AE Latent Space Realizations')\n",
    "plt.xlabel('AE Realization')\n",
    "plt.ylabel('AE Realization')\n",
    "plt.savefig('UnSorted Similarity Matrices.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Visualize sorted Jaccard matrix\n",
    "ordered_dist_mat, res_order, res_linkage = compute_serial_matrix(similarity_matrix,\"ward\")\n",
    "fig, ax1 = plt.subplots(1, 1)\n",
    "\n",
    "#subplot 1\n",
    "im1 = ax1.imshow(ordered_dist_mat, cmap='viridis')\n",
    "\n",
    "# Figure info\n",
    "ax1.set_aspect('auto')\n",
    "ax1.set_title('Sorted Jaccard Similarity Matrix for AE Realizations', size=12)\n",
    "ax1.set_xlabel('Sorted realization index, $i$', size=12)\n",
    "ax1.set_ylabel('Sorted realization index, $j$', size=12)\n",
    "ax1.tick_params(axis='both', which='major', labelsize=12)\n",
    "\n",
    "\n",
    "# Aesthetics\n",
    "axis = fig.add_axes([0.85, 0.30, 0.04, 1.0])  # Left,bottom, width, length\n",
    "cbar = fig.colorbar(im1, cax=axis, orientation='vertical')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.3, right=0.8, top=1.3, wspace=0.25, hspace=0.3)\n",
    "plt.savefig('Sorted Similarity Matrices.png', dpi=300, bbox_inches='tight')\n",
    "ax1.grid(visible=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Summary statistics on Box Plot"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr_changes = np.array(change_percentages)\n",
    "sorted_data = np.sort(arr_changes)\n",
    "\n",
    "# Calculate the 5-point statistic\n",
    "minimum = np.min(sorted_data)\n",
    "maximum = np.max(sorted_data)\n",
    "median = np.median(sorted_data)\n",
    "q1 = np.percentile(sorted_data, 25)\n",
    "q3 = np.percentile(sorted_data, 75)\n",
    "variance = np.var(sorted_data)\n",
    "\n",
    "# Display the five-number summary\n",
    "print(\"Minimum:\", round(minimum, 5))\n",
    "print(\"First Quartile (Q1):\", round(q1, 5))\n",
    "print(\"Median (Q2):\", round(median, 5))\n",
    "print(\"Third Quartile (Q3):\", round(q3, 5))\n",
    "print(\"Maximum:\", round(maximum, 5))\n",
    "print(\"Variance:\", round(variance, 5))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "statistics_dict = {\n",
    "    'Mean': statistics_array[:, 1],\n",
    "    'Pearson': statistics_array[:, 4],\n",
    "    'Spearman': statistics_array[:, 5],\n",
    "    'Moran': statistics_array[:, 6]\n",
    "}\n",
    "\n",
    "box_plot(dictionary=statistics_dict, var_name='Statistics', value_name='Values', save_title='Global statistics for all AE realization', box_width=0.6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pvalues_dict = {\n",
    "    'Pearson': pvalues_array[:, 0],\n",
    "    'Spearmann': pvalues_array[:, 1],\n",
    "    'Moran': pvalues_array[:, 2]\n",
    "}\n",
    "\n",
    "box_plot(dictionary=pvalues_dict, var_name='Statistics', value_name='p-values', save_title='p-values for all AE realizations', box_width=0.6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "other_dict = {'Entropy': statistics_array[:, 0],\n",
    "              'Variance': statistics_array[:, 2],\n",
    "             'Frobenius \\nNorm': statistics_array[:, 3]\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=other_dict, var_name='Statistics', value_name='Values', save_title='Other conventional statistics for all AE realizations', box_width=0.6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_mvee_anis = percentage_change(anisotropy_array[:,0], data_type='numpy')\n",
    "change_global_anis = percentage_change(anisotropy_array[:,1], data_type='numpy')\n",
    "change_arith_anis = percentage_change(anisotropy_array[:,2], data_type='numpy')\n",
    "change_har_anis = percentage_change(anisotropy_array[:,3], data_type='numpy')\n",
    "change_geo_anis = percentage_change(anisotropy_array[:,4], data_type='numpy')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract local anisotropies\n",
    "max_length = max(len(arr) for arr in local_anisotropies) # also yields the max number of arrays that the original list is seperated into\n",
    "element_arrays = [np.array([arr[i] if i < len(arr) else np.nan for arr in local_anisotropies]) for i in range(max_length)]\n",
    "\n",
    "anisotropy_dict = {\n",
    "              'MVEE ': anisotropy_array[:,0],\n",
    "              'Global ': anisotropy_array[:,1],\n",
    "              'Local 1 ': element_arrays[0],\n",
    "              'Local 2 ': element_arrays[1],\n",
    "              'Arithmetic ': anisotropy_array[:,2],\n",
    "              'Harmonic ': anisotropy_array[:,3],\n",
    "              'Geometric ': anisotropy_array[:,4]\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=anisotropy_dict, var_name='Anisotropies', value_name='Anisotropy', save_title='Anisotropy measures for all AE realizations', xlabel_rot=45)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_anisotropy_dict = {\n",
    "              'MVEE ': np.array(change_mvee_anis),\n",
    "              'Global ': np.array(change_global_anis),\n",
    "              'Arithmetic ': np.array(change_arith_anis),\n",
    "              'Harmonic ': np.array(change_har_anis),\n",
    "              'Geometric ': np.array(change_geo_anis)\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=change_anisotropy_dict, var_name='Anisotropies', value_name='Changes in Anisotropy (%)', save_title='Change in Anisotropy measures for all AE realizations')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "change_instability_dict = {\n",
    "              'Anchors \\nChanges': np.array(change_anchors),\n",
    "              'Truth Cluster \\nChanges': np.array(change_percentages),\n",
    "              'Global \\nAnisotropy': np.array(change_global_anis),\n",
    "              'Harmonic \\nAnisotropy': np.array(change_har_anis),\n",
    "              }\n",
    "\n",
    "box_plot(dictionary=change_instability_dict, var_name='Reassignments', value_name='Changes in Instability Measures (%)', save_title='Change in Instability measures for all AE realizations',\n",
    "         box_width=0.6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualize Anisotropies of different forms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='global', plotter=True)\n",
    "print(global_anis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Limitations of global anisotropy:\n",
    "\n",
    "Clustering: In datasets with multiple clusters, anisotropy might not accurately reflect the separation or relative positioning of these clusters. It may only indicate the overall spread without detailing the intra-cluster and inter-cluster distances.\n",
    "Density Variations: Anisotropy does not capture variations in density within the dataset. Areas of high data point concentration versus sparse regions are not differentiated by this metric.\n",
    "\n",
    "Fixed the above limitations by the introduction of geometric and/or harmonic anisotropy from  local anisotropy calculations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_anis, local_anis, arithmetic_anis, harmonic_anis, geometric_anis = compute_anisotropy(array_2d=encoded_data, type='local', plotter=True)\n",
    "print(local_anis, arithmetic_anis, harmonic_anis, geometric_anis)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
